{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd8Qjx2rq53Z",
        "colab_type": "code",
        "outputId": "cdc29428-6572-4e38-82dc-c8479474ef1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14SlBwDnrYhb",
        "colab_type": "code",
        "outputId": "ce1db662-e8bd-45ec-d499-4f1b191ae521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "root_path = 'drive/My Drive/Colab/'  #change dir to your project folder"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emlI8m2zdGCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from argparse import Namespace\n",
        "import collections\n",
        "from collections import Counter\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import csv\n",
        "import time\n",
        "import copy\n",
        "import json\n",
        "import string\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOCySPFc7N1Y",
        "colab_type": "text"
      },
      "source": [
        "# Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B5Y5Mn4efYa",
        "colab_type": "code",
        "outputId": "5c35433b-045d-4d26-c357-2b5e592e7b5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#%% ==================================== Set up ====================================\n",
        "# Set Numpy and PyTorch seeds\n",
        "def set_seeds(seed, cuda):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Random setting for DataLoader\n",
        "def _init_fn(seed):\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "# cudnn setting\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True    \n",
        "      \n",
        "# Creating directories\n",
        "def create_dirs(dirpath):\n",
        "    if not os.path.exists(dirpath):\n",
        "        os.makedirs(dirpath)\n",
        "        \n",
        "# Arguments\n",
        "args = Namespace(\n",
        "    seed=1234,\n",
        "    cuda=True,\n",
        "    shuffle=True,\n",
        "    vectorizer_file=\"vectorizer.json\",\n",
        "    model_state_file=\"model.pth\",\n",
        "    save_dir=\"rob\",\n",
        "    train_size=0.80,\n",
        "    val_size=0.10,\n",
        "    test_size=0.10,\n",
        "    use_med_embeddings=True,\n",
        "    pretrained_embeddings=None,\n",
        "    cutoff=25, # token must appear at least <cutoff> times to be in SequenceVocabulary\n",
        "    num_epochs=10,\n",
        "    early_stopping_criteria=5,\n",
        "    learning_rate=1e-3,\n",
        "    batch_size=64,\n",
        "    max_seq_len=9000,\n",
        "    embedding_dim=100,\n",
        "    rnn_hidden_dim=128,\n",
        "    hidden_dim=100,\n",
        "    num_layers=1,\n",
        "    bidirectional=False,\n",
        "    dropout_p=0.5,\n",
        ")\n",
        "\n",
        "# Set seeds\n",
        "set_seeds(seed=args.seed, cuda=args.cuda)\n",
        "\n",
        "# Create save dir\n",
        "create_dirs(args.save_dir)\n",
        "\n",
        "# Expand filepaths\n",
        "args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
        "args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
        "\n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "print(\"Using CUDA: {}\".format(args.cuda))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using CUDA: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLvUv3Ev7HHY",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fQNgfi3e67M",
        "colab_type": "code",
        "outputId": "644082e5-1cd6-4dba-f23b-0e1cec9c6ebb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#%% ==================================== Data ====================================\n",
        "csv.field_size_limit(100000000)\n",
        "path = \"/content/drive/My Drive/Colab/fulldata.csv\"\n",
        "df = pd.read_csv(path, usecols=['text', 'label_random', 'label_blind', 'label_ssz'], sep = '\\t', engine = 'python', encoding='utf-8')\n",
        "\n",
        "df.loc[df.label_random==1, 'label'] = 'random'\n",
        "df.loc[df.label_random==0, 'label'] = 'non-random'\n",
        "#df.loc[df.label_blind==1, 'label'] = 'blinded'\n",
        "#df.loc[df.label_blind==0, 'label'] = 'non-blinded'\n",
        "#df.loc[df.label_ssz==1, 'label'] = 'ssz'\n",
        "#df.loc[df.label_ssz==0, 'label'] = 'non-ssz'\n",
        "df.label.value_counts()\n",
        "\n",
        "# Split by category\n",
        "by_label = collections.defaultdict(list)\n",
        "for _, row in df.iterrows():\n",
        "    by_label[row.label].append(row.to_dict())\n",
        "for label in by_label:\n",
        "    print(\"{0}: {1}\".format(label, len(by_label[label])))\n",
        "    \n",
        "    \n",
        "# Create split data\n",
        "final_list = []\n",
        "for _, item_list in sorted(by_label.items()):\n",
        "    if args.shuffle:\n",
        "        np.random.shuffle(item_list)\n",
        "    n = len(item_list)\n",
        "    n_train = int(args.train_size*n)\n",
        "    n_val = int(args.val_size*n)\n",
        "    n_test = int(args.test_size*n)\n",
        "\n",
        "  # Give data point a split attribute\n",
        "    for item in item_list[:n_train]:\n",
        "        item['split'] = 'train'\n",
        "    for item in item_list[n_train:n_train+n_val]:\n",
        "        item['split'] = 'val'\n",
        "    for item in item_list[n_train+n_val:]:\n",
        "        item['split'] = 'test'  \n",
        "\n",
        "    # Add to final list\n",
        "    final_list.extend(item_list)    \n",
        "\n",
        "# df with split datasets\n",
        "split_df = pd.DataFrame(final_list)\n",
        "split_df[\"split\"].value_counts()\n",
        "\n",
        "# Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
        "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "#    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "#    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "#    text = re.sub(r'\\d+', '', text)\n",
        "#    text = re.sub(r\"[!%^&*()=_+{};:$Â£â‚¬@~#|/,.<>?\\`\\'\\\"\\[\\]\\\\]\", \" \", text)  # [!%^&*()=_+{};:$Â£â‚¬@~#|/<>?\\`\\'\\\"\\[\\]\\\\]\n",
        "#    text = re.sub(r'\\b(\\w{1})\\b', '', text) \n",
        "#    text = re.sub(r'\\s+', ' ', text)\n",
        "#    text.lower()\n",
        "    \n",
        "    \n",
        "split_df.text = split_df.text.apply(preprocess_text)\n",
        "#split_df.text[0]\n",
        "\n",
        "# Remove records with null text\n",
        "for i in range(len(split_df)):\n",
        "    if not split_df['text'][i]:\n",
        "        print(i)\n",
        "        split_df = split_df.drop([i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "random: 882\n",
            "non-random: 3350\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAqYAySI7UUU",
        "colab_type": "text"
      },
      "source": [
        "# Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3l8xLXUgILa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% ==================================== Vocabulary ====================================\n",
        "class Vocabulary(object):\n",
        "    def __init__(self, token_to_idx=None):\n",
        "\n",
        "        # Token to index\n",
        "        if token_to_idx is None:\n",
        "            token_to_idx = {}\n",
        "        self.token_to_idx = token_to_idx\n",
        "\n",
        "        # Index to token\n",
        "        self.idx_to_token = {idx: token \\\n",
        "                             for token, idx in self.token_to_idx.items()}\n",
        "\n",
        "    def to_serializable(self):\n",
        "        return {'token_to_idx': self.token_to_idx}\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        return cls(**contents)\n",
        "\n",
        "    def add_token(self, token):\n",
        "        if token in self.token_to_idx:\n",
        "            index = self.token_to_idx[token]\n",
        "        else:\n",
        "            index = len(self.token_to_idx)\n",
        "            self.token_to_idx[token] = index\n",
        "            self.idx_to_token[index] = token\n",
        "        return index\n",
        "\n",
        "    def add_tokens(self, tokens):\n",
        "        return [self.add_token[token] for token in tokens]\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        if token not in self.token_to_idx:\n",
        "            raise KeyError(\"the token (%s) is not in the Vocabulary\" % token)\n",
        "        return self.token_to_idx[token]\n",
        "\n",
        "    def lookup_index(self, index):\n",
        "        if index not in self.idx_to_token:\n",
        "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
        "        return self.idx_to_token[index]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_to_idx)\n",
        "\n",
        "# Test: Vocabulary instance\n",
        "#label_vocab = Vocabulary()\n",
        "#for index, row in df.iterrows():\n",
        "#    label_vocab.add_token(row.label)\n",
        "#print(label_vocab) # __str__\n",
        "#print(len(label_vocab)) # __len__\n",
        "#index = label_vocab.lookup_token(\"random\")\n",
        "#print(index)\n",
        "#print(label_vocab.lookup_index(index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH4Z6iza7Wh1",
        "colab_type": "text"
      },
      "source": [
        "# Sequence vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JltIOjirgLF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% ==================================== Sequence vocabulary ====================================\n",
        "class SequenceVocabulary(Vocabulary):\n",
        "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
        "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
        "                 end_seq_token=\"<END>\"):\n",
        "\n",
        "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
        "\n",
        "        self.mask_token = mask_token\n",
        "        self.unk_token = unk_token\n",
        "        self.begin_seq_token = begin_seq_token\n",
        "        self.end_seq_token = end_seq_token\n",
        "\n",
        "        self.mask_index = self.add_token(self.mask_token)\n",
        "        self.unk_index = self.add_token(self.unk_token)\n",
        "        self.begin_seq_index = self.add_token(self.begin_seq_token)\n",
        "        self.end_seq_index = self.add_token(self.end_seq_token)\n",
        "        \n",
        "        # Index to token\n",
        "        self.idx_to_token = {idx: token \\\n",
        "                             for token, idx in self.token_to_idx.items()}\n",
        "\n",
        "    def to_serializable(self):\n",
        "        contents = super(SequenceVocabulary, self).to_serializable()\n",
        "        contents.update({'unk_token': self.unk_token,\n",
        "                         'mask_token': self.mask_token,\n",
        "                         'begin_seq_token': self.begin_seq_token,\n",
        "                         'end_seq_token': self.end_seq_token})\n",
        "        return contents\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        return self.token_to_idx.get(token, self.unk_index)\n",
        "    \n",
        "    def lookup_index(self, index):\n",
        "        if index not in self.idx_to_token:\n",
        "            raise KeyError(\"the index (%d) is not in the SequenceVocabulary\" % index)\n",
        "        return self.idx_to_token[index]\n",
        "    \n",
        "    def __str__(self):\n",
        "        return \"<SequenceVocabulary(size=%d)>\" % len(self.token_to_idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_to_idx)\n",
        "\n",
        "\n",
        "## Test: Get word counts\n",
        "#word_counts = Counter()\n",
        "#for text in split_df.text:\n",
        "#    for token in text.split(\" \"):\n",
        "#        if token not in string.punctuation:\n",
        "#            word_counts[token] += 1\n",
        "#\n",
        "## Test: Create SequenceVocabulary instance\n",
        "#paper_vocab = SequenceVocabulary()\n",
        "#for word, word_count in word_counts.items():\n",
        "#    if word_count >= args.cutoff:\n",
        "#        paper_vocab.add_token(word)\n",
        "#print(paper_vocab) # __str__\n",
        "#print(len(paper_vocab)) # __len__\n",
        "#index = paper_vocab.lookup_token(\"general\")\n",
        "#print(index)\n",
        "#print(paper_vocab.lookup_index(index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChDcRogH7cFo",
        "colab_type": "text"
      },
      "source": [
        "# Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT-QR6zfgOVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% ==================================== Vectorizer ====================================\n",
        "class PapersVectorizer(object):\n",
        "    def __init__(self, paper_vocab, label_vocab):\n",
        "        self.paper_vocab = paper_vocab\n",
        "        self.label_vocab = label_vocab\n",
        "\n",
        "    def vectorize(self, paper):\n",
        "        indices = [self.paper_vocab.lookup_token(token) for token in paper.split(\" \")]\n",
        "        indices = [self.paper_vocab.begin_seq_index] + indices + [self.paper_vocab.end_seq_index]\n",
        "        \n",
        "        # Create vector\n",
        "        paper_length = len(indices)\n",
        "        vector = np.zeros(paper_length, dtype=np.int64)\n",
        "        vector[:len(indices)] = indices\n",
        "        return vector, paper_length\n",
        "    \n",
        "    def unvectorize(self, vector):\n",
        "        tokens = [self.paper_vocab.lookup_index(index) for index in vector]\n",
        "        paper = \" \".join(token for token in tokens)\n",
        "        return paper\n",
        "\n",
        "    @classmethod\n",
        "    def from_dataframe(cls, df, cutoff):\n",
        "        \n",
        "        # Create class vocab\n",
        "        label_vocab = Vocabulary()        \n",
        "        for label in sorted(set(df.label)):\n",
        "            label_vocab.add_token(label)\n",
        "\n",
        "        # Get word counts\n",
        "        word_counts = Counter()\n",
        "        for paper in df.text:\n",
        "            for token in paper.split(\" \"):\n",
        "                word_counts[token] += 1\n",
        "        \n",
        "        # Create paper vocab\n",
        "        paper_vocab = SequenceVocabulary()\n",
        "        for word, word_count in word_counts.items():\n",
        "            if word_count >= cutoff:\n",
        "                paper_vocab.add_token(word)\n",
        "        \n",
        "        return cls(paper_vocab, label_vocab)\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        paper_vocab = SequenceVocabulary.from_serializable(contents['paper_vocab'])\n",
        "        label_vocab = Vocabulary.from_serializable(contents['label_vocab'])\n",
        "        return cls(paper_vocab=paper_vocab, label_vocab=label_vocab)\n",
        "    \n",
        "    def to_serializable(self):\n",
        "        return {'paper_vocab': self.paper_vocab.to_serializable(),\n",
        "                'label_vocab': self.label_vocab.to_serializable()}\n",
        "\n",
        "## Test: Vectorizer instance\n",
        "#vectorizer = PapersVectorizer.from_dataframe(split_df, cutoff=args.cutoff)\n",
        "#print(vectorizer.paper_vocab)\n",
        "#print(vectorizer.label_vocab)\n",
        "#vectorized_paper, paper_length = vectorizer.vectorize(preprocess_text(\"Roger Federer wins the Wimbledon tennis tournament.\"))\n",
        "#print(np.shape(vectorized_paper))\n",
        "#print(\"paper_length:\", paper_length)\n",
        "#print(vectorized_paper)\n",
        "#print(vectorizer.unvectorize(vectorized_paper))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KJPw-3e7fzJ",
        "colab_type": "text"
      },
      "source": [
        "# Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idX-UFSSgS9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% ==================================== Dataset class ====================================\n",
        "class PapersDataset(Dataset):\n",
        "    def __init__(self, df, vectorizer):\n",
        "        self.df = df\n",
        "        self.vectorizer = vectorizer\n",
        "\n",
        "        # Data splits\n",
        "        self.train_df = self.df[self.df.split=='train']\n",
        "        self.train_size = len(self.train_df)\n",
        "        self.val_df = self.df[self.df.split=='val']\n",
        "        self.val_size = len(self.val_df)\n",
        "        self.test_df = self.df[self.df.split=='test']\n",
        "        self.test_size = len(self.test_df)\n",
        "        self.lookup_dict = {'train': (self.train_df, self.train_size), \n",
        "                            'val': (self.val_df, self.val_size),\n",
        "                            'test': (self.test_df, self.test_size)}\n",
        "        self.set_split('train')\n",
        "\n",
        "        # Class weights (for imbalances)\n",
        "        class_counts = df.label.value_counts().to_dict()\n",
        "        def sort_key(item):\n",
        "            return self.vectorizer.label_vocab.lookup_token(item[0])\n",
        "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
        "        frequencies = [count for _, count in sorted_counts]\n",
        "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_make_vectorizer(cls, df, cutoff):\n",
        "        train_df = df[df.split=='train']\n",
        "        return cls(df, PapersVectorizer.from_dataframe(train_df, cutoff))\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_load_vectorizer(cls, df, vectorizer_filepath):\n",
        "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
        "        return cls(df, vectorizer)\n",
        "\n",
        "    def load_vectorizer_only(vectorizer_filepath):\n",
        "        with open(vectorizer_filepath) as fp:\n",
        "            return PapersVectorizer.from_serializable(json.load(fp))\n",
        "\n",
        "    def save_vectorizer(self, vectorizer_filepath):\n",
        "        with open(vectorizer_filepath, \"w\") as fp:\n",
        "            json.dump(self.vectorizer.to_serializable(), fp)\n",
        "\n",
        "    def set_split(self, split=\"train\"):\n",
        "        self.target_split = split\n",
        "        self.target_df, self.target_size = self.lookup_dict[split]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Dataset(split={0}, size={1})\".format(self.target_split, self.target_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.target_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.target_df.iloc[index]\n",
        "        paper_vector, paper_length = self.vectorizer.vectorize(row.text)\n",
        "        label_index = self.vectorizer.label_vocab.lookup_token(row.label)\n",
        "        return {'paper': paper_vector, \n",
        "                'paper_length': paper_length, \n",
        "                'label': label_index}\n",
        "\n",
        "    def get_num_batches(self, batch_size):\n",
        "        return len(self) // batch_size\n",
        "\n",
        "    def generate_batches(self, batch_size, collate_fn, shuffle=True, drop_last=False, device=\"cpu\"):\n",
        "        dataloader = DataLoader(dataset=self, batch_size=batch_size,\n",
        "                                collate_fn=collate_fn, shuffle=shuffle, \n",
        "                                drop_last=drop_last,\n",
        "                                num_workers = 0,\n",
        "                                worker_init_fn=_init_fn)\n",
        "        for data_dict in dataloader:\n",
        "            out_data_dict = {}\n",
        "            for name, tensor in data_dict.items():\n",
        "                out_data_dict[name] = data_dict[name].to(device)\n",
        "            yield out_data_dict\n",
        "\n",
        "\n",
        "## Test: Dataset instance\n",
        "#dataset = PapersDataset.load_dataset_and_make_vectorizer(df=split_df, cutoff=args.cutoff)\n",
        "##dataset.save_vectorizer(args.vectorizer_file)\n",
        "##vectorizer = PapersDataset.load_vectorizer_only(vectorizer_filepath=args.vectorizer_file)\n",
        "#print(dataset) # __str__\n",
        "#input_ = dataset[5] # __getitem__\n",
        "#print(input_['paper'], input_['paper_length'], input_['label'])\n",
        "#print(dataset.vectorizer.unvectorize(input_['paper']))\n",
        "#print(dataset.class_weights) # tensor([0.0003, 0.0011])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1co9SouY7i1j",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lpf3zqDogp_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% ==================================== Model ====================================\n",
        "def gather_last_relevant_hidden(hiddens, x_lengths):\n",
        "    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
        "    out = []\n",
        "    for batch_index, column_index in enumerate(x_lengths):\n",
        "        out.append(hiddens[batch_index, column_index])\n",
        "    return torch.stack(out)\n",
        "\n",
        "class PapersModel(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_embeddings, rnn_hidden_dim, \n",
        "                 hidden_dim, output_dim, num_layers, bidirectional, dropout_p, \n",
        "                 pretrained_embeddings=None, freeze_embeddings=False, \n",
        "                 padding_idx=0):\n",
        "        super(PapersModel, self).__init__()\n",
        "        \n",
        "        if pretrained_embeddings is None:\n",
        "            self.embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
        "                                           num_embeddings=num_embeddings,\n",
        "                                           padding_idx=padding_idx)\n",
        "        else:\n",
        "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "            self.embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
        "                                           num_embeddings=num_embeddings,\n",
        "                                           padding_idx=padding_idx,\n",
        "                                           _weight=pretrained_embeddings)\n",
        "        \n",
        "        # Conv weights\n",
        "        self.gru = nn.GRU(input_size=embedding_dim, hidden_size=rnn_hidden_dim, \n",
        "                          num_layers=num_layers, batch_first=True, \n",
        "                          bidirectional=bidirectional)\n",
        "     \n",
        "        # FC weights\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc1 = nn.Linear(rnn_hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "        if freeze_embeddings:\n",
        "            self.embeddings.weight.requires_grad = False\n",
        "\n",
        "    def forward(self, x_in, x_lengths, apply_softmax=False):\n",
        "        \n",
        "        # Embed\n",
        "        x_in = self.embeddings(x_in)\n",
        "            \n",
        "        # Feed into RNN\n",
        "        out, h_n = self.gru(x_in)\n",
        "        \n",
        "        # Gather the last relevant hidden state\n",
        "        out = gather_last_relevant_hidden(out, x_lengths)\n",
        "\n",
        "        # FC layers\n",
        "        z = self.dropout(out)\n",
        "        z = self.fc1(z)\n",
        "        z = self.dropout(z)\n",
        "        y_pred = self.fc2(z)\n",
        "\n",
        "        if apply_softmax:\n",
        "            y_pred = F.softmax(y_pred, dim=1)\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIpmaAmz7lLp",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnANHy9egysD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% ==================================== Training ====================================\n",
        "class Trainer(object):\n",
        "    def __init__(self, dataset, model, model_state_file, save_dir, device, shuffle, \n",
        "               num_epochs, batch_size, learning_rate, early_stopping_criteria):\n",
        "        self.dataset = dataset\n",
        "        self.class_weights = dataset.class_weights.to(device)\n",
        "        self.model = model.to(device)\n",
        "        self.save_dir = save_dir\n",
        "        self.device = device\n",
        "        self.shuffle = shuffle\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.loss_func = nn.CrossEntropyLoss(self.class_weights)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer=self.optimizer, mode='min', factor=0.5, patience=1)\n",
        "        self.train_state = {\n",
        "            'done_training': False,\n",
        "            'stop_early': False, \n",
        "            'early_stopping_step': 0,\n",
        "            'early_stopping_best_val': 1e8,\n",
        "            'early_stopping_criteria': early_stopping_criteria,\n",
        "            'learning_rate': learning_rate,\n",
        "            'epoch_index': 0,\n",
        "            \n",
        "            'train_loss': [],\n",
        "            'train_acc': [],\n",
        "            'train_sens': [],\n",
        "            'train_spec': [],\n",
        "            'train_prec': [],\n",
        "            'train_f1': [],\n",
        "            \n",
        "            'val_loss': [],\n",
        "            'val_acc': [],\n",
        "            'val_sens': [],\n",
        "            'val_spec': [],\n",
        "            'val_prec': [],\n",
        "            'val_f1': [],\n",
        "            \n",
        "            'test_loss': -1,\n",
        "            'test_acc': -1,\n",
        "            'test_sens': -1,\n",
        "            'test_spec': -1,\n",
        "            'test_prec': -1,\n",
        "            'test_f1': -1,\n",
        "            \n",
        "            'model_filename': model_state_file}\n",
        "    \n",
        "    def update_train_state(self):\n",
        "\n",
        "        # Verbose\n",
        "        print(\"[Epoch{0} / Train] | LR {1} | loss: {2:.3f} | accuracy: {3:.2f}% | sensitivity: {4:.2f}% | specificity: {5:.2f}% | precision: {6:.2f}% | f1: {7:.2f}%\".format(\n",
        "            self.train_state['epoch_index'], self.train_state['learning_rate'], \n",
        "            self.train_state['train_loss'][-1], self.train_state['train_acc'][-1], \n",
        "            self.train_state['train_sens'][-1], self.train_state['train_spec'][-1], \n",
        "            self.train_state['train_prec'][-1], self.train_state['train_f1'][-1]))\n",
        "        \n",
        "        print(\"[Epoch{0} / Val] | LR {1} | loss: {2:.3f} | accuracy: {3:.2f}% | sensitivity: {4:.2f}% | specificity: {5:.2f}% | precision: {6:.2f}% | f1: {7:.2f}%\".format(\n",
        "            self.train_state['epoch_index'], self.train_state['learning_rate'], \n",
        "            self.train_state['val_loss'][-1], self.train_state['val_acc'][-1], \n",
        "            self.train_state['val_sens'][-1], self.train_state['val_spec'][-1], \n",
        "            self.train_state['val_prec'][-1], self.train_state['val_f1'][-1]))\n",
        "\n",
        "        # Save one model at least\n",
        "        if self.train_state['epoch_index'] == 0:\n",
        "            torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
        "            self.train_state['stop_early'] = False\n",
        "\n",
        "        # Save model if performance improved\n",
        "        elif self.train_state['epoch_index'] >= 1:\n",
        "            loss_tm1, loss_t = self.train_state['val_loss'][-2:]\n",
        "\n",
        "            # If loss worsened\n",
        "            if loss_t >= self.train_state['early_stopping_best_val']:\n",
        "                # Update step\n",
        "                self.train_state['early_stopping_step'] += 1\n",
        "\n",
        "            # Loss decreased\n",
        "            else:\n",
        "                # Save the best model\n",
        "                if loss_t < self.train_state['early_stopping_best_val']:\n",
        "                    torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
        "\n",
        "                # Reset early stopping step\n",
        "                self.train_state['early_stopping_step'] = 0\n",
        "\n",
        "            # Stop early ?\n",
        "            self.train_state['stop_early'] = self.train_state['early_stopping_step'] \\\n",
        "              >= self.train_state['early_stopping_criteria']\n",
        "        return self.train_state\n",
        "  \n",
        "    def compute_accuracy(self, y_pred, y_target):\n",
        "        _, y_pred_indices = y_pred.max(dim=1)\n",
        "        n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
        "        return n_correct / len(y_pred_indices) * 100\n",
        "    \n",
        "    def compute_sensitivity(self, y_pred, y_target):\n",
        "        _, y_pred_indices = y_pred.max(dim=1)\n",
        "        temp = torch.eq(y_pred_indices, torch.ones_like(y_target)) & torch.eq(y_target, torch.ones_like(y_target))\n",
        "        tp = temp.sum().item()\n",
        "        temp = torch.eq(y_pred_indices, torch.zeros_like(y_target)) & torch.eq(y_target, torch.ones_like(y_target))\n",
        "        fn = temp.sum().item()\n",
        "        if tp+fn == 0:\n",
        "            sensitivity = 0\n",
        "        else:\n",
        "            sensitivity = 100 * tp / (tp+fn)\n",
        "        return sensitivity  \n",
        "    \n",
        "    def compute_specificity(self, y_pred, y_target):\n",
        "        _, y_pred_indices = y_pred.max(dim=1)\n",
        "        temp = torch.eq(y_pred_indices, torch.zeros_like(y_target)) & torch.eq(y_target, torch.zeros_like(y_target))\n",
        "        tn = temp.sum().item()\n",
        "        temp = torch.eq(y_pred_indices, torch.ones_like(y_target)) & torch.eq(y_target, torch.zeros_like(y_target))\n",
        "        fp = temp.sum().item()\n",
        "        if tn+fp == 0:\n",
        "            specificity = 0\n",
        "        else:\n",
        "            specificity = 100 * tn / (tn+fp)\n",
        "        return specificity \n",
        "    \n",
        "    def compute_precision(self, y_pred, y_target):\n",
        "        _, y_pred_indices = y_pred.max(dim=1)\n",
        "        temp = torch.eq(y_pred_indices, torch.ones_like(y_target)) & torch.eq(y_target, torch.ones_like(y_target))\n",
        "        tp = temp.sum().item()\n",
        "        temp = torch.eq(y_pred_indices, torch.ones_like(y_target)) & torch.eq(y_target, torch.zeros_like(y_target))\n",
        "        fp = temp.sum().item()\n",
        "        if tp+fp == 0:\n",
        "            precision = 0\n",
        "        else:\n",
        "            precision = 100 * tp / (tp+fp) \n",
        "        return precision\n",
        "    \n",
        "    def compute_f1(self, y_pred, y_target):\n",
        "        _, y_pred_indices = y_pred.max(dim=1)\n",
        "        temp = torch.eq(y_pred_indices, torch.ones_like(y_target)) & torch.eq(y_target, torch.ones_like(y_target))\n",
        "        tp = temp.sum().item()\n",
        "        temp = torch.eq(y_pred_indices, torch.ones_like(y_target)) & torch.eq(y_target, torch.zeros_like(y_target))\n",
        "        fp = temp.sum().item()\n",
        "        temp = torch.eq(y_pred_indices, torch.zeros_like(y_target)) & torch.eq(y_target, torch.ones_like(y_target))\n",
        "        fn = temp.sum().item()\n",
        "        if 2*tp+fp+fn == 0:\n",
        "            f1 = 0\n",
        "        else:\n",
        "            f1 = 100 * 2*tp / (2*tp+fp+fn)\n",
        "        return f1 \n",
        "    \n",
        "    \n",
        "    def pad_seq(self, seq, length):\n",
        "        vector = np.zeros(length, dtype=np.int64)\n",
        "        if len(seq) <= length:\n",
        "            vector[:len(seq)] = seq\n",
        "            vector[len(seq):] = self.dataset.vectorizer.paper_vocab.mask_index\n",
        "        else:\n",
        "            vector = seq[:length]  \n",
        "        return vector\n",
        "  \n",
        "    \n",
        "    def collate_fn(self, batch):\n",
        "        \n",
        "        # Make a deep copy\n",
        "        batch_copy = copy.deepcopy(batch)\n",
        "#        processed_batch = {\"paper\": [], \"paper_length\": [], \"label\": []}\n",
        "        processed_batch = {\"paper\": [], \"paper_pad_len\": [], \"label\": []}\n",
        "        \n",
        "        # Get max sequence length\n",
        "        max_seq_len = args.max_seq_len \n",
        "        \n",
        "        # Pad\n",
        "        for i, sample in enumerate(batch_copy):\n",
        "            padded_seq = self.pad_seq(sample[\"paper\"], max_seq_len)\n",
        "            processed_batch[\"paper\"].append(padded_seq)\n",
        "            processed_batch[\"paper_pad_len\"].append(max_seq_len)\n",
        "#            processed_batch[\"paper_length\"].append(sample[\"paper_length\"])\n",
        "            processed_batch[\"label\"].append(sample[\"label\"])\n",
        "        \n",
        "        # Convert to appropriate tensor types\n",
        "        processed_batch[\"paper\"] = torch.LongTensor(processed_batch[\"paper\"])\n",
        "        processed_batch[\"paper_pad_len\"] = torch.LongTensor(processed_batch[\"paper_pad_len\"])\n",
        "        processed_batch[\"label\"] = torch.LongTensor(processed_batch[\"label\"])\n",
        "        \n",
        "        return processed_batch   \n",
        "  \n",
        "    def run_train_loop(self):\n",
        "        for epoch_index in range(self.num_epochs):\n",
        "            self.train_state['epoch_index'] = epoch_index\n",
        "      \n",
        "            ###### Iterate over train dataset ######\n",
        "\n",
        "            # initialize batch generator, set loss and acc to 0, set train mode on\n",
        "            self.dataset.set_split('train')\n",
        "            batch_generator = self.dataset.generate_batches(batch_size=self.batch_size, \n",
        "                                                            collate_fn=self.collate_fn, \n",
        "                                                            shuffle=self.shuffle, device=self.device)          \n",
        "            running_loss = 0.0\n",
        "            running_acc = 0.0\n",
        "            running_sens = 0.0\n",
        "            running_spec = 0.0\n",
        "            running_prec = 0.0\n",
        "            running_f1 = 0.0\n",
        "            \n",
        "            self.model.train()\n",
        "\n",
        "            for batch_index, batch_dict in enumerate(batch_generator):\n",
        "                # zero the gradients\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # compute the output\n",
        "                y_pred = self.model(batch_dict['paper'], batch_dict['paper_pad_len'])\n",
        "\n",
        "                # compute the loss\n",
        "                loss = self.loss_func(y_pred, batch_dict['label'])\n",
        "                loss_t = loss.item()\n",
        "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "                # compute gradients using loss\n",
        "                loss.backward()\n",
        "\n",
        "                # use optimizer to take a gradient step\n",
        "                self.optimizer.step()\n",
        "                \n",
        "                # compute the accuracy\n",
        "                acc_t = self.compute_accuracy(y_pred, batch_dict['label'])\n",
        "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "                \n",
        "                # compute the sensitivity\n",
        "                sens_t = self.compute_sensitivity(y_pred, batch_dict['label'])\n",
        "                running_sens += (sens_t - running_sens) / (batch_index + 1)\n",
        "\n",
        "                # compute the specificity\n",
        "                spec_t = self.compute_specificity(y_pred, batch_dict['label'])\n",
        "                running_spec += (spec_t - running_spec) / (batch_index + 1)\n",
        "                \n",
        "                # compute the precision\n",
        "                prec_t = self.compute_precision(y_pred, batch_dict['label'])\n",
        "                running_prec += (prec_t - running_prec) / (batch_index + 1)\n",
        "                \n",
        "                # compute the f1 score\n",
        "                f1_t = self.compute_f1(y_pred, batch_dict['label'])\n",
        "                running_f1 += (f1_t - running_f1) / (batch_index + 1)\n",
        "\n",
        "\n",
        "            self.train_state['train_loss'].append(running_loss)\n",
        "            self.train_state['train_acc'].append(running_acc)\n",
        "            self.train_state['train_sens'].append(running_sens)\n",
        "            self.train_state['train_spec'].append(running_spec)\n",
        "            self.train_state['train_prec'].append(running_prec)\n",
        "            self.train_state['train_f1'].append(running_f1)\n",
        "            \n",
        "            ###### Iterate over val dataset ######\n",
        "\n",
        "            # # initialize batch generator, set loss and acc to 0; set eval mode on\n",
        "            self.dataset.set_split('val')\n",
        "            batch_generator = self.dataset.generate_batches(batch_size=self.batch_size, \n",
        "                                                            collate_fn=self.collate_fn, \n",
        "                                                            shuffle=self.shuffle, device=self.device)\n",
        "            running_loss = 0.0\n",
        "            running_acc = 0.0\n",
        "            running_sens = 0.0\n",
        "            running_spec = 0.0\n",
        "            running_prec = 0.0\n",
        "            running_f1 = 0.0\n",
        "            \n",
        "            self.model.eval()\n",
        "\n",
        "            for batch_index, batch_dict in enumerate(batch_generator):\n",
        "\n",
        "                # compute the output\n",
        "                y_pred =  self.model(batch_dict['paper'], batch_dict['paper_pad_len'])\n",
        "\n",
        "                # compute the loss\n",
        "                loss = self.loss_func(y_pred, batch_dict['label'])\n",
        "                loss_t = loss.to(\"cpu\").item()\n",
        "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "                # compute the accuracy\n",
        "                acc_t = self.compute_accuracy(y_pred, batch_dict['label'])\n",
        "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "                \n",
        "                # compute the sensitivity\n",
        "                sens_t = self.compute_sensitivity(y_pred, batch_dict['label'])\n",
        "                running_sens += (sens_t - running_sens) / (batch_index + 1)\n",
        "                \n",
        "                # compute the specificity\n",
        "                spec_t = self.compute_specificity(y_pred, batch_dict['label'])\n",
        "                running_spec += (spec_t - running_spec) / (batch_index + 1)\n",
        "                \n",
        "                # compute the precision\n",
        "                prec_t = self.compute_precision(y_pred, batch_dict['label'])\n",
        "                running_prec += (prec_t - running_prec) / (batch_index + 1)\n",
        "                \n",
        "                # compute the f1 score\n",
        "                f1_t = self.compute_f1(y_pred, batch_dict['label'])\n",
        "                running_f1 += (f1_t - running_f1) / (batch_index + 1)\n",
        "\n",
        "            self.train_state['val_loss'].append(running_loss)\n",
        "            self.train_state['val_acc'].append(running_acc)\n",
        "            self.train_state['val_sens'].append(running_sens)\n",
        "            self.train_state['val_spec'].append(running_spec)\n",
        "            self.train_state['val_prec'].append(running_prec)\n",
        "            self.train_state['val_f1'].append(running_f1)\n",
        "            \n",
        "            self.train_state = self.update_train_state()\n",
        "            self.scheduler.step(self.train_state['val_loss'][-1])\n",
        "            if self.train_state['stop_early']:\n",
        "                break\n",
        "    \n",
        "      \n",
        "    def run_test_loop(self):\n",
        "        # initialize batch generator, set loss and acc to 0; set eval mode on\n",
        "        self.dataset.set_split('test')\n",
        "        batch_generator = self.dataset.generate_batches(batch_size=self.batch_size, \n",
        "                                                        collate_fn=self.collate_fn, \n",
        "                                                        shuffle=self.shuffle, device=self.device)\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        running_sens = 0.0\n",
        "        running_spec = 0.0\n",
        "        running_prec = 0.0\n",
        "        running_f1 = 0.0\n",
        "        \n",
        "        self.model.eval()\n",
        "\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            # compute the output\n",
        "            y_pred =  self.model(batch_dict['paper'], batch_dict['paper_pad_len'])\n",
        "\n",
        "            # compute the loss\n",
        "            loss = self.loss_func(y_pred, batch_dict['label'])\n",
        "            loss_t = loss.item()\n",
        "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "            # compute the accuracy\n",
        "            acc_t = self.compute_accuracy(y_pred, batch_dict['label'])\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "        self.train_state['test_loss'] = running_loss\n",
        "        self.train_state['test_acc'] = running_acc\n",
        "        self.train_state['test_sens'] = running_sens\n",
        "        self.train_state['test_spec'] = running_spec\n",
        "        self.train_state['test_prec'] = running_prec\n",
        "        self.train_state['test_f1'] = running_f1\n",
        "        \n",
        "    \n",
        "    def plot_performance(self):\n",
        "        # Figure size\n",
        "        plt.figure(figsize=(15,5))\n",
        "\n",
        "        # Plot Loss\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.title(\"Loss\")\n",
        "        plt.plot(trainer.train_state[\"train_loss\"], label=\"train\")\n",
        "        plt.plot(trainer.train_state[\"val_loss\"], label=\"val\")\n",
        "        plt.legend(loc='upper right')\n",
        "\n",
        "        # Plot Accuracy\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.title(\"Accuracy\")\n",
        "        plt.plot(trainer.train_state[\"train_acc\"], label=\"train\")\n",
        "        plt.plot(trainer.train_state[\"val_acc\"], label=\"val\")\n",
        "        plt.legend(loc='lower right')\n",
        "\n",
        "        # Save figure\n",
        "        plt.savefig(os.path.join(self.save_dir, \"performance.png\"))\n",
        "\n",
        "        # Show plots\n",
        "        plt.show()\n",
        "    \n",
        "    def save_train_state(self):\n",
        "        self.train_state[\"done_training\"] = True\n",
        "        with open(os.path.join(self.save_dir, \"train_state.json\"), \"w\") as fp:\n",
        "            json.dump(self.train_state, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QifrZKFV7pKo",
        "colab_type": "text"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVANVPpzzhLU",
        "colab_type": "code",
        "outputId": "1f6934ac-d477-4dd6-f852-84333afa883e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "w2v_path = '/content/drive/My Drive/Colab/wikipedia-pubmed-and-PMC-w2v.bin'\n",
        "med_w2v = KeyedVectors.load_word2vec_format(w2v_path, binary=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6wxEvWIg1jy",
        "colab_type": "code",
        "outputId": "f685cc68-0bed-4a4c-af7a-b041a66dde23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        }
      },
      "source": [
        "#%% ==================================== Main ====================================            \n",
        "###### Using embeddings ######\n",
        "#w2v_path = '/content/drive/My Drive/Colab/wikipedia-pubmed-and-PMC-w2v.bin'\n",
        "#med_w2v = KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
        "\n",
        "def make_embeddings_matrix(words):\n",
        "    embedding_dim = args.embedding_dim\n",
        "    embeddings = np.zeros((len(words), embedding_dim))\n",
        "    for i, word in enumerate(words):\n",
        "        if word in med_w2v:\n",
        "            embedding_vector = med_w2v[word]\n",
        "            embeddings[i, :] = embedding_vector[:embedding_dim]            \n",
        "        else:\n",
        "            embedding_i = torch.zeros(1, embedding_dim)\n",
        "            nn.init.xavier_uniform_(embedding_i)\n",
        "            embeddings[i, :] = embedding_i\n",
        "    return embeddings\n",
        "\n",
        "args.use_med_embeddings = True    \n",
        "\n",
        "\n",
        "###### Initialization ######\n",
        "dataset = PapersDataset.load_dataset_and_make_vectorizer(df=split_df, cutoff=args.cutoff)\n",
        "dataset.save_vectorizer(args.vectorizer_file)\n",
        "vectorizer = dataset.vectorizer\n",
        "\n",
        "# Create embeddings\n",
        "embeddings = None\n",
        "if args.use_med_embeddings:\n",
        "    words = vectorizer.paper_vocab.token_to_idx.keys()\n",
        "    embeddings = make_embeddings_matrix(words=words)\n",
        "    print (\"<Embeddings(words={0}, dim={1})>\".format(np.shape(embeddings)[0], np.shape(embeddings)[1])) \n",
        "\n",
        "del(med_w2v)\n",
        "\n",
        "# Initialize model \n",
        "model = PapersModel(embedding_dim=args.embedding_dim, \n",
        "                    num_embeddings=len(vectorizer.paper_vocab), \n",
        "                    rnn_hidden_dim=args.rnn_hidden_dim,\n",
        "                    hidden_dim=args.hidden_dim,\n",
        "                    output_dim=len(vectorizer.label_vocab),\n",
        "                    num_layers=args.num_layers,\n",
        "                    bidirectional=args.bidirectional,\n",
        "                    dropout_p=args.dropout_p, \n",
        "                    pretrained_embeddings=embeddings, \n",
        "                    padding_idx=vectorizer.paper_vocab.mask_index)\n",
        "print(model.named_modules)\n",
        "\n",
        "\n",
        "###### Train ######\n",
        "trainer = Trainer(dataset=dataset, model=model, \n",
        "                  model_state_file=args.model_state_file, \n",
        "                  save_dir=args.save_dir, device=args.device,\n",
        "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
        "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
        "                  early_stopping_criteria=args.early_stopping_criteria)\n",
        "\n",
        "start_time = time.time()\n",
        "trainer.run_train_loop()\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "\n",
        "# Test performance\n",
        "trainer.run_test_loop()\n",
        "print (\"[Test] loss: {0:.3f} | accuracy: {1:.2f}% | sensitivity: {2:.2f}% | specificity: {3:.2f}% | precision: {4:.2f}% | f1: {5:.2f}%\".format(\n",
        "         trainer.train_state['test_loss'], trainer.train_state['test_acc'], \n",
        "         trainer.train_state['test_sens'], trainer.train_state['test_spec'], \n",
        "         trainer.train_state['test_prec'], trainer.train_state['test_f1']))\n",
        "print('Time elapsed: {0:.1f} minutes\\n'.format(elapsed_time/60))\n",
        "\n",
        "\n",
        "# Plot performance\n",
        "trainer.plot_performance()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Embeddings(words=31879, dim=100)>\n",
            "<bound method Module.named_modules of PapersModel(\n",
            "  (embeddings): Embedding(31879, 100, padding_idx=0)\n",
            "  (gru): GRU(100, 128, batch_first=True)\n",
            "  (dropout): Dropout(p=0.5)\n",
            "  (fc1): Linear(in_features=128, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=2, bias=True)\n",
            ")>\n",
            "[Epoch0 / Train] | LR 0.001 | loss: 0.697 | accuracy: 52.40% | sensitivity: 40.73% | specificity: 55.40% | precision: 18.87% | f1: 21.74%\n",
            "[Epoch0 / Val] | LR 0.001 | loss: 0.703 | accuracy: 78.85% | sensitivity: 0.89% | specificity: 99.26% | precision: 14.29% | f1: 1.68%\n",
            "[Epoch1 / Train] | LR 0.001 | loss: 0.695 | accuracy: 59.42% | sensitivity: 37.92% | specificity: 65.55% | precision: 22.00% | f1: 23.24%\n",
            "[Epoch1 / Val] | LR 0.001 | loss: 0.695 | accuracy: 74.99% | sensitivity: 3.32% | specificity: 93.95% | precision: 17.14% | f1: 5.24%\n",
            "[Epoch2 / Train] | LR 0.001 | loss: 0.692 | accuracy: 57.10% | sensitivity: 44.45% | specificity: 60.42% | precision: 23.29% | f1: 29.50%\n",
            "[Epoch2 / Val] | LR 0.001 | loss: 0.698 | accuracy: 73.36% | sensitivity: 7.29% | specificity: 90.85% | precision: 18.85% | f1: 9.93%\n",
            "[Epoch3 / Train] | LR 0.001 | loss: 0.689 | accuracy: 59.69% | sensitivity: 43.88% | specificity: 64.07% | precision: 24.97% | f1: 29.61%\n",
            "[Epoch3 / Val] | LR 0.001 | loss: 0.698 | accuracy: 66.83% | sensitivity: 13.10% | specificity: 81.53% | precision: 14.75% | f1: 12.60%\n",
            "[Epoch4 / Train] | LR 0.001 | loss: 0.678 | accuracy: 60.80% | sensitivity: 48.72% | specificity: 64.47% | precision: 28.45% | f1: 33.10%\n",
            "[Epoch4 / Val] | LR 0.001 | loss: 0.703 | accuracy: 70.30% | sensitivity: 9.54% | specificity: 85.88% | precision: 15.71% | f1: 11.72%\n",
            "[Epoch5 / Train] | LR 0.001 | loss: 0.650 | accuracy: 59.11% | sensitivity: 54.92% | specificity: 59.93% | precision: 29.74% | f1: 32.70%\n",
            "[Epoch5 / Val] | LR 0.001 | loss: 0.719 | accuracy: 72.17% | sensitivity: 8.12% | specificity: 88.79% | precision: 15.93% | f1: 10.64%\n",
            "[Epoch6 / Train] | LR 0.001 | loss: 0.623 | accuracy: 66.95% | sensitivity: 46.51% | specificity: 72.72% | precision: 37.48% | f1: 35.79%\n",
            "[Epoch6 / Val] | LR 0.001 | loss: 0.744 | accuracy: 74.97% | sensitivity: 10.02% | specificity: 91.84% | precision: 22.14% | f1: 13.36%\n",
            "[Epoch7 / Train] | LR 0.001 | loss: 0.594 | accuracy: 66.59% | sensitivity: 51.33% | specificity: 70.07% | precision: 40.09% | f1: 38.14%\n",
            "[Epoch7 / Val] | LR 0.001 | loss: 0.765 | accuracy: 75.56% | sensitivity: 7.94% | specificity: 92.36% | precision: 22.73% | f1: 11.26%\n",
            "[Epoch8 / Train] | LR 0.001 | loss: 0.573 | accuracy: 61.92% | sensitivity: 56.18% | specificity: 63.38% | precision: 29.15% | f1: 37.31%\n",
            "[Epoch8 / Val] | LR 0.001 | loss: 0.785 | accuracy: 32.22% | sensitivity: 88.06% | specificity: 18.05% | precision: 21.29% | f1: 33.96%\n",
            "[Epoch9 / Train] | LR 0.001 | loss: 0.564 | accuracy: 61.11% | sensitivity: 63.85% | specificity: 60.54% | precision: 37.04% | f1: 39.98%\n",
            "[Epoch9 / Val] | LR 0.001 | loss: 0.812 | accuracy: 73.51% | sensitivity: 9.72% | specificity: 90.11% | precision: 19.32% | f1: 12.41%\n",
            "[Test] loss: 0.873 | accuracy: 74.06% | sensitivity: 0.00% | specificity: 0.00% | precision: 0.00% | f1: 0.00%\n",
            "Time elapsed: 28.2 minutes\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3QAAAE/CAYAAAAOkIE9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xd4lFXax/HvSa8kpIeEGkroLYJY\nsCtWWHUFC4tr26Juc4vbdXV33eKu+i7uLioWrFjRtfeKdBQSOtICKYQkkJCe8/5xJhCQEiDJM5P8\nPtc118w8ZeYeLmDmfs45922stYiIiIiIiEjgCfI6ABERERERETk6SuhEREREREQClBI6ERERERGR\nAKWETkREREREJEApoRMREREREQlQSuhEREREREQClBI6ERERERGRAKWETqQNGGM2GGPO9DoOERGR\ntmaM+cAYU2qMCfc6FpHOSAmdiIiIiBwVY0wv4GTAAhe14/uGtNd7ifg7JXQi7cgYc70xZq0xZocx\n5mVjTDffdmOM+acxpsgYs9MYs8wYM8S37zxjTJ4xZpcxJt8Y81NvP4WIiMge3wI+Bx4BpjVtNMZE\nGmPuNsZsNMaUG2M+McZE+vadZIz5zBhTZozZbIy52rf9A2PMdc1e42pjzCfNnltjzI3GmDXAGt+2\ne32vsdMYs8gYc3Kz44ONMb8yxqzzfYcuMsZ0N8ZMN8bc3fxD+L6Tf9wWf0AibU0JnUg7McacDvwZ\nuAxIBzYCT/t2nw2MB/oDcb5jSnz7HgK+Y62NBYYA77Vj2CIiIofyLeAJ3+0cY0yqb/vfgdHACUAC\n8HOg0RjTE3gd+D8gGRgBLD2C95sEjAUG+Z4v8L1GAvAk8KwxJsK37yfA5cB5QBfgGmA38ChwuTEm\nCMAYkwSc6TtfJOAooRNpP1cCM621i621NcAvgXG+6Sp1QCyQDRhr7Qpr7TbfeXXAIGNMF2ttqbV2\nsQexi4iI7MMYcxLQE5htrV0ErAOu8CVK1wA/tNbmW2sbrLWf+b77rgDesdY+Za2ts9aWWGuPJKH7\ns7V2h7W2CsBa+7jvNeqttXcD4cAA37HXAb+x1q6yzhe+Y+cD5cAZvuOmAB9YawuP8Y9ExBNK6ETa\nTzfcqBwA1toK3ChchrX2PeBfwHSgyBgzwxjTxXfoJbirixuNMR8aY8a1c9wiIiIHMg14y1q73ff8\nSd+2JCACl+Dtr/tBtrfU5uZPjDE/Ncas8E3rLMPNcklqwXs9Clzle3wVMOsYYhLxlBI6kfazFXcl\nEwBjTDSQCOQDWGvvs9aOxk0j6Q/8zLd9gbV2IpACvATMbue4RURE9uFbD3cZcIoxpsAYUwD8GBiO\nW1ZQDWQd4NTNB9kOUAlENXuedoBjbLMYTsZN5bwM6GqtjceNvJkWvNfjwERjzHBgIO77VSQgKaET\naTuhxpiIphvwFPBtY8wIX2nnPwHzrLUbjDHHGWPGGmNCcV9o1bi1BmHGmCuNMXHW2jpgJ9Do2ScS\nERFxJgENuIuQI3y3gcDHuHV1M4F/GGO6+YqTjPN99z0BnGmMucwYE2KMSTTGjPC95lLgYmNMlDGm\nL3DtYWKIBeqBYiDEGPM73Fq5Jg8Cdxhj+vmKjw0zxiQCWGu34NbfzQKeb5rCKRKIlNCJtJ3XgKpm\nt1OB3wLPA9twVw2n+I7tAjwAlOKmZZYAf/PtmwpsMMbsBL6LW4snIiLipWnAw9baTdbagqYbbvnA\nlcCtwDJc0rQD+AsQZK3dhFtGcItv+1LcqB7AP4FaoBA3JfKJw8TwJvAGsBr33VnNvlMy/4Gb1fIW\n7oLoQ0Bks/2PAkPRdEsJcMZae/ijREREREQ6EGPMeNzUy55WP4glgGmETkREREQ6Fd8Shx8CDyqZ\nk0CnhE5EREREOg1jzECgDFe85R6PwxE5ZppyKSIiIiIiEqA0QiciIiIiIhKglNCJiIiIiIgEqBCv\nA9hfUlKS7dWrl9dhiIhIO1i0aNF2a22y13EECn1Hioh0Dkfy/eh3CV2vXr1YuHCh12GIiEg7MMZs\n9DqGQKLvSBGRzuFIvh815VJERKSNGWN+bIzJNcYsN8Y8ZYyJMMb0NsbMM8asNcY8Y4wJ8zpOEREJ\nPEroRERE2pAxJgP4AZBjrR0CBANTgL8A/7TW9gVKgWu9i1JERAKVEjoREZG2FwJEGmNCgChgG3A6\n8Jxv/6PAJI9iExGRAOZ3a+hERDqTuro6tmzZQnV1tdehtKmIiAgyMzMJDQ31OpR2Z63NN8b8HdgE\nVAFvAYuAMmttve+wLUCGRyGKiEgAU0InIuKhLVu2EBsbS69evTDGeB1Om7DWUlJSwpYtW+jdu7fX\n4bQ7Y0xXYCLQGygDngUmHMH5NwA3APTo0aMtQhQRkQCmKZciIh6qrq4mMTGxwyZzAMYYEhMTO/wo\n5CGcCXxlrS221tYBLwAnAvG+KZgAmUD+gU621s6w1uZYa3OSk9XhQURE9qWETkTEYx05mWvSGT7j\nIWwCjjfGRBn3B3EGkAe8D1zqO2YaMMej+EREJIApoRMR6cTKysq4//77j/i88847j7KysjaIqOOx\n1s7DFT9ZDCzDfffOAH4B/MQYsxZIBB7yLEgREQlYSuhERDqxgyV09fX1Bzh6r9dee434+Pi2CqvD\nsdb+3lqbba0dYq2daq2tsdaut9aOsdb2tdZ+01pb43WcIiISeJTQiYh0Yrfeeivr1q1jxIgRHHfc\ncZx88slcdNFFDBo0CIBJkyYxevRoBg8ezIwZM9xJ1tKrZw+2FxWwYcMGBg4cyPXXX8/gwYM5++yz\nqaqq8vATySHt+AoWPwYNh07YRUQkcCihExHpxO666y6ysrJYunQpf/vb31i8eDH33nsvq1evBmDm\nzJksWrSIhQsXct9991FSmA/bV0NjPVSVA7BmzRpuvPFGcnNziY+P5/nnn/fyI8mhLH0SXr4Z/j0O\n8l4Ga72OSEREjpHaFoiI+InbX8klb+vOVn3NQd268PsLB7f4+DFjxuzTWuC+++7jxRdfBGDz5k2s\nWfAeiaOHQ1AIRCdCRQW9e/dmxIgRAIwePZoNGza06meQVnTaryB9GLx7B8yeChmj4YzfQ59TvI5M\nRESOkkboRERkj+jo6D2PP/jgA9555x3mfvIRX7z3AiMH96e6IQiSs8Hs/foIDw/f8zg4OPiw6+/E\nQ8bAwAvhe5/BxOmwqxAeuwgemwRbl3gdnYiIHAWN0ImI+IkjGUlrLbGxsezateuA+8rLy+naJYao\nio2sXLOOzxcvhy7dICSsnaOUVhccAiOvgiGXwoIH4eO7YcapMPgbcNpvIKmv1xGKiEgLKaETEenE\nEhMTOfHEExkyZAiRkZGkpqa6HY0NTBg7kP/cu4uBp3yDAdmDOP74490Ij3QcoRFwwk0waip89i+Y\nO92trRs1FU75hUvgRUTErxnrZwuic3Jy7MKFC70OQ0SkXaxYsYKBAwd6Hca+aiqgbCM01EJMKsSm\n7TPF8mgd6LMaYxZZa3OO+cU7iTb/jqwogo/+DgtnQlAwjP0unPQjiOzadu8pIiJfcyTfj1pDJyIi\njm2E8nwoWQMYSOrvRmhaIZmTABGTAuf9FW5eCIMmwaf3wr3D4eN/QO1ur6MTEZED0Le0iIhAXRUU\nr4bKIohKhOQBEBZ9+POkY+raCy7+L3z3E+gxDt69He4bCQsegoY6r6MTEZFmWpTQGWMmGGNWGWPW\nGmNuPcD+HsaY940xS4wxXxpjzmu275e+81YZY85pzeBFROQYWQsVhVC8ChrrIKEPxPdw0+1E0obA\nFc/At99wSd6rP4HpY2D589DY6HV0IiJCCxI6Y0wwMB04FxgEXG6MGbTfYb8BZltrRwJTgPt95w7y\nPR8MTADu972eiIh4rb4GStbCzq0Q0cW1I4iI8zoq8Uc9x8E1b8AVsyEkEp67BmacAmvfUXNyERGP\ntWSEbgyw1lq73lpbCzwNTNzvGAt08T2OA7b6Hk8EnrbW1lhrvwLW+l5PRES8Yi3sLoHilVC3243I\nde0NwaFeRyb+zBjofw5892P4xgyoLoPHL4FHL4TNC7yOTkSk02pJQpcBbG72fItvW3O3AVcZY7YA\nrwE3H8G5IiLSXhrqofQrKNsEoVFuVC4qUe0IpOWCgmH4ZLhpEZz7N3dh4KEz4ekroWil19GJiHQ6\nrVUU5XLgEWttJnAeMMuYlpdFM8bcYIxZaIxZWFxc3EohiYjIPqrLoXgFVO901SsT+0JI+BG9RExM\nTBsFJwEnJAzG3gA/WOqaka//EP49Dl66Eco2H/58ERFpFS1JuvKB7s2eZ/q2NXctMBvAWjsXiACS\nWngu1toZ1toca21OcnJyy6MXEZHDa2xwP7B3rIegEFfBMiZVo3LSOsJj4JSfwQ+/gOO/D8uehf8b\nBW/8CipLvI5ORKTDa0lCtwDoZ4zpbYwJwxU5eXm/YzYBZwAYYwbiErpi33FTjDHhxpjeQD9gfmsF\nLyIih1Fb6SpY7t4O0SkumQuN3LP71ltvZfr06Xue33bbbdx5552cccYZjBo1iqFDhzJnzhwvIpdA\nE50I5/wRbl4Ewy6Def92Pew++ItrVi8iIm3isAmdtbYeuAl4E1iBq2aZa4z5gzHmIt9htwDXG2O+\nAJ4CrrZOLm7kLg94A7jRWtvQFh9ERESasY2wcxtsXw1YSOwHcRlfaxI+efJkZs+evef57NmzmTZt\nGi+++CKLFy/m/fff55ZbbsGqkqG0VHx3mDgdvv85ZJ0KH/zJJXbz/usqq4qISKsKaclB1trXcMVO\nmm/7XbPHecCJBzn3j8AfjyFGEZHO4fVboWDZsb+ObYT6arANkDoELvq/g/aVGzlyJEVFRWzdupXi\n4mK6du1KWloaP/7xj/noo48ICgoiPz+fwsJC0tLSjj026TySB8Dkx2HLQnjnNnj95zD3X3Dar2Ho\nN9XrUESklbQooRMRkUBgoaEOGmrd05BICO9y2B/O3/zmN3nuuecoKChg8uTJPPHEExQXF7No0SJC\nQ0Pp1asX1dXV7RC/dEiZOTDtFVj3Hrx7O7z4Hfj0Pjjjd64NgtZyirSvuirXuqZyu5uOX1ni7vds\na75vO0QluBH3IyyiJe1HCZ2IiL84966jP7ehFko3Qe0ul8TF92hxX7nJkydz/fXXs337dj788ENm\nz55NSkoKoaGhvP/++2zcuPHo4xIBl7T1PQP6nAZ5L8F7d8JTk6H78XDmba5xuYgcOWuhZqcvAdux\nNwnbc1/y9eStrvLAr2WCXRub6CR3nzbUrX9d+zZsXwNpQ9r3s0mLKaETEQl0VaW+MvEW4rofcV+5\nwYMHs2vXLjIyMkhPT+fKK6/kwgsvZOjQoeTk5JCdnd12sUvnEhQEQy6GgRfCksfhg7vg4QnQ7xw3\nYqcfjNLZNTa4/9MPmZQ1S952l+ydlbG/kMi9yVlUIiT1g6gkV8AoKqlZ8ubbFh7n/o02V5jrErri\nlfr36ceU0ImIBKrGeijbAtWlrkl4154QEnFUL7Vs2d61e0lJScydO/eAx1VUqFqhtILgUMj5Ngyb\nDPNnwCf/gP+cBOnDIHMMdB8L3Y+D+J6akikdW80uWP4CfPGUK2K1ewdwkCJU4XF7k7H47tBtuC8Z\nS2p2n7g3UQuLPvb4Evu6kbvilcf+WtJmlNCJiASi6p1QtskldbHp6isngSksCk76EYyeBvMfhA0f\nwdInYcEDbn9MKmQe50vwxkD6CAg9uosWIn7DWtj0uRulzn3RTYFMzoZBk76elDUlapEJEBLW/rGG\nhENiFhStaP/3lhZTQiciEkgaG2HXVqgsdl+0Cf3dj2KRQBbZ1TUnP+Vn0FAPRXmwZT5s9t1W/s8d\nFxQK6cNdcteU6MVleBu7SEvtKnQjcUseh5I1EBYDQy+Bkd9yxYP89aJccrb7Nyl+SwmdiEigqN0N\nZRtdS4LoZIjt9vX1DiKBLjjETb1MHwbHXee2VRTvTfC2LICFM+Hz+92+Lhm+BG+Mu08b5s1IhsiB\nNNS7NWiLZ8HqN1w7me7Hu5HpQZMgPMbrCA8vOdtdVKmr1gi5n1JCJyLiMWst5lBXZq2FikLYVQBB\nIZCQBRFd2i/AVqDG5HJMYpIh+3x3A9eeo2CZL8GbD5sXuKlr4NaRpo9wyV1Toheb6l3s0jmVrIMl\ns2DpU1BR4C7CjbsRRk6F5P5eR3dkUrJdf9OSNa7ypfgdJXQiIh6KiIigpKSExMTEAyd19dWuHUFd\nJUR0hfhMl9QFEGstJSUlREToyq60kuBQyBjlbnzXbdu5bd9pmvP+A5/d5/bF99x3FC91iBsJFGlN\ntbshb45L5DZ+CiYI+p3tkrj+57S4lYzfSR7o7otWKqHzU/rfTETEQ5mZmWzZsoXi4uJ9d1jrespV\n73TPIxMgrAoK17R/kK0gIiKCzMxMr8OQjqxLOgya6G4A9TWw7QtfgjcPNnwCy551+0KjIGO0bx2e\nL9GLTvQudglc1sLWxW5K5fLnXU+4hD6uDcfwK9zfy0CX2NddSFSlS7+lhE5ExEOhoaH07t1774aG\nevjiSdefa2c+9D0LLrxXhR9EjlRI+N5pl9zkfniXb3HJ3ZYFLtH77D5XKRbcVOamdgndx7p1Q0HB\nnn4E8WO7d8CXz7hErijX9XwbNBFGTYWeJ/pvgZOjERLm/n0ooWuZCl/RsnZcGqGETkTEH1jrpuq8\nd6dbp5CRA9/4D/Qe73VkIh2DMa53V3x3GHqp21a7G7Yt3TtNc+3b7oIKQFgsZI52yV3/Cb7pndKp\nNTbC+vfdlMqVr7qG3t1Gwvn/cH+nIuK8jrDtpGRDwXKvowgMH94FXz4Lt25st8ReCZ2IiNfWvQ/v\n3g5bl7hRgSlPwoDzOtYVXhF/FBYFPU9wN3AXVkq/ckVWtviman70N3e74B7XL086n7JNsOQJWPoE\nlG92bTZyrnFr49KGeB1d+0jOhhWvqNJlSxTmQcrAdv0OV0InIuKVLYvg3dvgq48grgdM+g8Mu0zT\nvES8Yoxb/5TQB4ZPdtuqyuC5a+CVH0BFEYz/qS62dAb1Na5U/+JZsP4Dt63PqXDW7TDg/M6X1CT7\nKl1uX+1aisiBWeum4A6+uF3fVgmdiEh7K1oJ793hfixEJcG5f4XRV7s59yLiXyLj4YpnYM5N8P6d\nrgT9uX/VhZeOqmC5m1L55TNQVQpx3eGUX8DIKyG+h9fReSfFV+myeKUSukPZuRWqyyF1cLu+rRI6\nEZH2UrbJFTv54ikIi4HTfg3Hfw/CY72OTEQOJTgUJv0bYlJcIZWKIrj4gc43StNRVZfDsudcIrd1\nCQSHuZ6HI6e6UTkl764oiipdHl5RnrtPGdSub6uETkSkrVUUw8d3w8KHAAPHfx9O+onKpIsEkqAg\nOPsOiE2DN38Fj++AKU+4ETwJPNa6XnGLZ7mCVPVVkDIYJtwFQy/T/8/7Cwlz7QuKlNAdUmGuu09V\nQici0jFU74S5/4K506Guyk3ZOeUXEKd+bCIBa9yNEJMKL34XHj4Prnq+Y/Qa6yx2bnOVTJc8DjvW\nQ3gXGD7FtRvoNkrrIw8lORsKvvQ6Cv9WlAex3VzhnHakhE5EpLXVVcOCB92oXNUOGDQJTv8NJPXz\nOjIRaQ1DL4WoBHhmKjx0Flz1AiT39zoqOZDaStj0uSs+9dVHrk2FbXS94sb/3PWOC4vyOsrAkJzt\nRjPrqiA00uto/FNhXruPzoESOhGR1rN/U/Cs0+GM37k+RSLSsWSdDlf/D574Jsw8B66Y7ZqSi7fq\na1zj+KYEbstCaKxz678ycmD8z2DYZEjM8jrSwJOSDVhfpcvhXkfjfxrqYPsqyDqt3d9aCZ2IyLFS\nU3CRzqnbSLj2LZh1MTx6IVz2KPQ/x+uoOpeGejfq9tWHLoHb9DnUV4MJcknHuO+7/4u7Hw/hMV5H\nG9iSfZUui1YqoTuQknWu2Xw7V7gEJXQiIsdGTcFFOreEPi6pe+JSeOpyuOg+GHmV11F1XI2NULh8\n7wjcxs+gdpfblzIYRn8bep/smsW38zqmDi8xC4JCVenyYIp8BVHaucIlKKETETk6agouIk1iUuDq\nV+GZq2DOjVBR6CrZ6sLOsbMWtq/ZOwK34RO3NhlcKf2hl7oRuF4nQ0yyt7F2dMGhrtKlEroDK8wD\nEwzJA9r9rZXQiYgcCTUFF5EDCY+FK56Fl74H7/4BdhW6EvhBQV5HFnhKN/hG4D529xUFbnuXTBhw\n7t4ELi7D0zA7pZRs2LrU6yj8U1GeS3g9+D2ghE5EpCXUFFxEDickzDUcj0mFz6dDZRF847+64HM4\nO7fBho/3jsKVbXLbo1Pc9Mne492ta2+NenotORtyX4La3aoOur/CXMgY5clbK6ETETkUNQUXkSMR\nFAQT/uQakL/9W6jc7hqQR8R5HZn/2L3Dl8D51sFtX+22R8RDr5Ng3M0ugUseoATO3yQ3q3TZbYTX\n0fiPml1QthFGTvXk7ZXQiYgciJqCSysxxgwAnmm2qQ/wO+Ax3/ZewAbgMmttaXvHJ23kxB+4tXVz\nboSHz4ernnNJXmdUvdMVL2lK4AqXue1hMa54ycipLoFLG6p1yP4uxVfpsnilErrminzrCj3oQQdK\n6ERE9qWm4NLKrLWrgBEAxphgIB94EbgVeNdae5cx5lbf8194Fqi0vuFT3Frb2d/yNSB/EZL6eh1V\n26urhk1z9yZwW5eAbYDgcOgx1v2f2vsU1/YhONTraOVIJPRRpcsD8bDCJSihExFx1BRc2scZwDpr\n7UZjzETgVN/2R4EPUELX8fQ7E65+xdeA/Gy48lnIGO11VG2juhzmPwCf3w+7S/Y28z75J24ELnMM\nhEZ4HaUci+BQd4GzSAndPgrzIDQa4nt68vZK6ESkc7MWVrwM796hpuDSHqYAT/kep1prt/keFwCp\n3oQkbS5jNFz7Nsz6BjxyAVw2yyV6HUXldpfEzX8AanZC37NgzPXQ80Q18+6IkrNh62Kvo/AvRXlu\nOqpHVW2V0IlI5/XVx/DO7yF/kfuCmvwEZJ+vRfjSJowxYcBFwC/332ettcYYe5DzbgBuAOjRo0eb\nxihtKDHLJXVPXAJPTYaJ092UzEBWnu/WGi96xK01HnQRnHwLpA/3OjJpS8nZkPuiKl02sdZVuBx4\ngWchKKETkc6nYBm8czusfRu6ZMDE+90PKy3Gl7Z1LrDYWlvoe15ojEm31m4zxqQDRQc6yVo7A5gB\nkJOTc8CkTwJEbCpc/Ro8cyW8+B3XgPyEHwTeRaQd6+GTe2Dpk2AbYdhkOOnHkNzf68ikPaQ0Vbpc\npWUJ4P4dV+2AlMGehaCETkQ6j9KN8P6f4MtnXAnxs+5w04JCI72OTDqHy9k73RLgZWAacJfvfo4X\nQUk7i+gCVz7nErq3f+cakJ99Z2A0IC/Mg0/+Acufd4UxRk9zCWlXb9YNiUeSfZUui1YqoQM33RI8\nq3AJSuhEpDOoLHFVKxc8ACYITvwhnPQjiOzqdWTSSRhjooGzgO8023wXMNsYcy2wEbjMi9jEAyHh\ncMnMvQ3IKwph0r9dY3J/lL8IProbVr3qCj+MuxHG3dR52zB0dgl9IDgMild4HYl/KPQldBqhExFp\nA7WV8Pm/4dN7obYCRlwJp/4S4jK8jkw6GWttJZC437YSXNVL6YyCgmDCXS6pe/d22L0dJj8O4bFe\nR+ZYCxs+cRfD1r/vmn6fciuM/Q5EJXgdnXgpOAQS+0HxKq8j8Q9Fee7fcXTi4Y9tI0roRKTjaaiH\nJbNcC4KKAsi+wLUgSB7gdWQiInsZ40r6x6TCyzfDI+e76ZgxKd7FZC2seRs+/jtsngfRKXDWHyDn\nGv9JNsV7KdmwZaHXUfiHwlzP+s81UUInIh2HtbDiFXe1u2QtdD8eLnvMNbIVEfFXI6+E6GR4dpqv\nAfkLripme2pscC1cPr7bFY6K6w7n/R1GXqV1xvJ1ydluLWVtJYRFex2NdxobXJP1467zNAwldCLS\nMWz4BN7+PeQvdF80lz8N/ScEXvU4Eemc+p8N05oakJ/jGpC3R8GJhjr4cjZ88k/XizOxr6v8O+wy\n10Ra5ECSs9198SrIGOVtLF7a8RXUV2uETkTkmBQsdyNya97ytSCYDsMvVwsCEQk8mTlw7Vsw62LX\ngHzyLMg6vW3eq64Kljzu1hiXb4bUofDNR2DgRfr/Uw4vxVfpsnhl507oinLdvYcVLkEJnYgEqrJN\nrgXBF0+7MuBn/QHG3KCpQSIS2JL6uaTu8Uvgictc9cth32y916/ZBQsegrnTobIIuo+F8/8B/c7S\njAZpua69XaXLok5e6bIwz1XPbhqx9IgSOhEJLLt3uDUe82cABk78gWtoqxYEItJRdEmHb78GT18J\nL1zna0B+07G95u4dMO+/MO8/UF0GfU6Dk2dCr5OUyMmRCw6BpP6qdFmU69o4eHwxuUUJnTFmAnAv\nEAw8aK29a7/9/wRO8z2NAlKstfG+fQ3AMt++Tdbai1ojcBHpZGp3w7x/wyf3+FoQXOFrQZDpdWQi\nIq0vMh6ueh5evAHe+rWr2HvmH468AfmuApj7L1gwE+oqXdXfk34CmaPbJm7pPJKzYct8r6PwVmGe\n59MtoQUJnTEmGJiOa4i6BVhgjHnZWpvXdIy19sfNjr8ZaL6Kt8paO6L1QhaRTqWhHpY+Du//2f2g\nGXCea0HQNH9fRKSjCo2ASx+G138Bn/0fVBS5dcItKVZSutGtj1vyODTWwZBLXCLnBz8+pYNIzobl\nz0FNBYTHeB1N+6vdDTvWw9BWnBJ9lFoyQjcGWGutXQ9gjHkamAjkHeT4y4Hft054ItJpWQsr/wfv\n3O4qr3Uf6xbs9xzndWQiIu0nKBjO+xvEpsF7d0DldteO5WA/oItXwyf/cJUrTZCbzXDiD9u/DYJ0\nfCm+dWPbV0FGJxzxLV4JWL+4SNKShC4D2Nzs+RbggE2djDE9gd7Ae802RxhjFgL1wF3W2peOMlYR\n6Sw2fArv/B62LICkATDlSTcyp3UeItIZGQPjf+oakL/yQ3j0ArjiWYhJ3nvMti/c+uK8lyEkAsZ+\nB8bdBHEZ3sUtHVuyb6ZM0crOmdAV+ca2UgZ7GwetXxRlCvCctbah2bae1tp8Y0wf4D1jzDJr7brm\nJxljbgBuAOjRo0crhyQiAaOkBSo+AAAgAElEQVQwz7UgWP0GxHaDi/4Phl/hFl+LiHR2o6b6GpBf\nDTPPdg3IKwrho7/D2rchvAuc/BM4/vsQneR1tHIEVhbs5KviSqLCQ4gJDyY6PITosBB3Hx5MeIgf\ntpJI6A3B4VDcSStdFuZBSKT7c/BYS34l5QPdmz3P9G07kCnAjc03WGvzfffrjTEf4NbXrdvvmBnA\nDICcnBzbksBFpAMp2+xrQfCU+0Fy5m0w5jsQFuV1ZCIi/mXABJj2Mjx5Gdx/vGtqHJUIp/8WxlwP\nEXFeRyhH6MPVxVz36ALqGg7+Ezg02DRL8lzCFxMeQlTY3scH2xa93/OosGDCQ4IwxzrrJSi4c1e6\nLMqF5AF+0bexJQndAqCfMaY3LpGbAlyx/0HGmGygKzC32bauwG5rbY0xJgk4EfhrawQuIh3AnhYE\nD7jnJ9zkFu1HJXgbl4iIP+s+Bq55E978FWSdAaOnQVi011HJUVi0sZTvzlpE35RY/nrJMGrqG6is\nbaCypp6Kmnoqa+rZXduw53FFTT27axqorHWPC3dWU+l7XllTf8iksLmQILNPstc8GdyTCPr2ZyZE\ncuGwboQEH6DCako2bJrXyn8qAaIwz/Vv9AOHTeistfXGmJuAN3FtC2Zaa3ONMX8AFlprX/YdOgV4\n2lrb/G/SQOC/xphGIAi3hu5gxVREpLOo3e16IX1yD9Ts3NuCIL774c8VERE3MnDV815HIcdgVcEu\nrnlkAaldwnnsmjEkx4Yf82vW1Dewu8aXAPqSvMqafRPEpoTRbfM99h1bvKvGJY217rzahkYAZn6y\ngb9cMoxB3brs+4bJA2DZs52v0mXldqgsghTvC6JAC9fQWWtfA17bb9vv9nt+2wHO+wwYegzxiUhH\n0lAPS5+AD/4Mu7ZB/3NdCwI/qBAlIiLSXjaV7GbqQ/OICA1i1rVjWyWZAwgPcevtukaHtcrr1dY3\n8lZeAbe9nMtF//qE75zSh5tP70dEqG+aYVNhlOJVnau3YWGuu/eT3y+qNCAi7WPde66X0vbVkDkG\nLp0JPU/wOioREZF2VbSrmqkz51Hb0Mjs74yje4L/rhcPCwnigmHdODEriTtfXcH099fx+vIC7rp4\nGGN6J+ztCVu8onMldH5U4RLcNEgRkbazewe8+D2Y9Q1obIDJT8C1bymZExGRTqe8qo5vPTSf4l01\nPHz1cfRPjfU6pBbpGh3G3ZcN57FrxlBb38hl/53Lr19cxq7IDNcmo6iTVboszHXFiGJSvI4E0Aid\niLQVayH3BTcqV1UKJ/8Uxv8MQiO8jkxERKTdVdU2cO0jC1hXXMHMq49jZI+uXod0xMb3T+bNH43n\n7rdW8/BnX/HeyiLeiulNbGerdFmU59bP+Ul/XI3QiUjrK8+Hpy6H566BuEy44QM447dK5kREpFOq\na2jk+08sYvGmUu6dMpKT+yUf/iQ/FR0ewu8uHMQL3zuBLhGhvFOSwI4NX7K9osbr0NpHY6Nrpp7q\nH9MtQQmdiLSmxkZY8CBMHwvrP4Cz/wjXvgNpqo0kIiKdU2Oj5afPfsH7q4r54zeGct7QdK9DahUj\ne3TllZtPIrXPcBLqC7no7jd4ftEW9i143wGVbYC6Sr+pcAlK6ESktRSvhkfOg1dvcQujvz/X9ZUL\n1sxuERHpnKy13PZKLnOWbuXnEwZw+ZgeXofUqsJCgjhh3EkAnBy/nVue/YJpDy9g847dHkfWhgp9\nBVE0QiciHUZ9LXz0N/jPiW5R9MT7YepLkNDb68hEREQ89c931vDY3I3cML4P3zsly+tw2kZyNgB/\nPimU2y8azKINOzjnno+Y+clXNDR2wNG6pgqXvs/tD5TQicjRy18EM06F9+6E7PPhpgUw8kq/WSQs\nIiLilYc//Yr73l3DZTmZ/PLcbExH/W7s2gtCIgjavoppJ/TirZ+cwpjeCfzhf3lc8u/PWF24y+sI\nW1dhrvvMftRIXQmdiBy52kp441fw4JlQtQOmPAXffMRvyveKiIh46YXFW7j9lTzOGZzKn74xtOMm\ncwBBwZDUH4pXApARH8nDVx/HPZNHsLGkkvPv+5h/vr2amvoGjwNtJUV5ftN/rokSOhE5Muveg/vH\nwefTYfS34cZ5kH2e11GJiIj4hXfyCvnZc19yQlYi904ZSUhwJ/i5nTLQVX70McYwaWQG7/zkFM4b\nms69767hgvs+YdHGUg+DbAV11VCyDlL9pyAKKKETkZZq3iA8OBS+/Tpc8A+IiPM6MhEREb8wb30J\nNz65mMHdujDjWzlEhAZ7HVL7SB4AO7dA9c59NifGhHPvlJHMvDqHypp6Lv3PZ9z2ci6VNfUeBXqM\ntq8G2+BXFS5BCZ2IHI61sPx5mD4Gls12DcK/+yn0PMHryERERPzG8vxyrnt0IZldI3nk22OICe9E\nVZ6TB7r7gzQYPz07lbd+cgpTj+/Jo3M3cPY/P+KDVUXtF19rKfK/CpeghE5EDkUNwkVERA7rq+2V\nXP3wfGIjQph17VgSosO8Dql9pfgqPhavOOghMeEh/GHiEJ79zjgiQoO4+uEF/PiZpeyorG2nIFtB\nYS4Eh0OCf1UsVUInIl+nBuEiIiItsq28iqsenIe1MOu6sXSLj/Q6pPYX3wtCIvdZR3cwOb0SePUH\nJ3Pz6X155YutnPWPD5mzND8wGpIX5UFyf7/rsauETkT2pQbhIiIiLVJaWcu3HppPeVUdj14zhqxk\n/yll366CglyiU3z4hA4gIjSYW84ewCs3n0Rm10h++PRSrnt0IVvLqto40GNU6H8VLkEJnYg0qa+F\nD9UgXEREpCUqauq5+pEFbNyxmwen5TAko5MXCUse2OKErsnA9C688P0T+c35A/l03XbO/udHzJq7\ngUZ/bEheVQq7tvpdhUtQQicisLdB+PtqEC4iInI4NfUNfHfWIpbnlzP9ilEc3yfR65C8lzwAduZD\ndfkRnRYcZLju5D689aNTGNE9nt/OyWXyjLmsLapoo0CPUqGvIIpG6ETEr+zTILxUDcJFREQOo6HR\n8uNnlvLJ2u385ZJhnDUo1euQ/EPKoStdHk6PxChmXTuGv106jNWFFZx378f867011DU0tmKQx2BP\nhUuN0ImIv1j7Ltx/fLMG4Z+rQbiIiMghWGv59YvLeG1ZAb85fyCXjs70OiT/keyrdFl08EqXh2OM\n4Zs53Xn7J+M5a1Aqf39rNRf+3yd8uaWslYI8BoW5EBEPseleR/I1SuhEOpumBuGPXwzBYWoQLiIi\n0kJ/eWMVTy/YzE2n9eW6k/t4HY5/ie8JoVFHvI7uQFJiI5h+5ShmTB1N6e5aJk3/lD++mkdVbUMr\nBHqUivJcQ3E/XI6isnUinYW1kPsCvP4LN73y5J/C+J+pp5yIiEgL/PfDdfznw3VcObYHt5zd3+tw\n/E9QECS1vNJlS5w9OI2xfRK56/WVPPDxV7yZW8ifLx7KiX2TWu09WsRaN/I47LL2fd8W0gidSGew\nT4Pw7nDDh2oQLiIi0kLPLNjEn19fyQXD0vnDxCEYPxyl8QspA1vUi+5IxEWG8ueLh/LU9ccTZODK\nB+fxs2e/oHx3Xau+zyGVb4aanW6Ezg9phE6kI2tshEUz4e3boLHeNQg//nsQFOx1ZCIiIgHhjeXb\n+OULyxjfP5l/XDaC4CAlcweVPAC+eAqqyiAyvlVfelxWIm/8aDz3vLOGBz5ez/urijlrUAppXSJJ\nj48gPS6C9LhI0uMiiA5v5RSnqcJlqv9VuAQldCIdV/FqeOUHsGku9DkVLrhHPeVERESOwKdrt/OD\np5Yyons8/7lqFGEhmtx2SMnNKl32GNvqLx8RGsyt52ZzwbB0/vTaCt7OK2R7Re3XjouNCKFbXCRp\ncRF0i4/YL+lzid8RJX1Fue6+qZKnn1FCJ9LR1NfCp/fCR391i5Mn/RuGX+6Xi3hFRET81Reby7jh\nsYX0Topm5tXHERWmn82HleKrdFm8ok0SuiZDMuJ48vrjAdcTsLC8hm3lVWwrr2ZbeTUF5VVsLa+m\noLya3K3lh036mo/uNSV+aXGRxDQlfYV5bsmKnxaQ099MkY6iqhSWPgkLHoQd62HwxXDuX9RTTkRE\n5AitLdrF1Q/PJyEmjFnXjiE+KszrkAJDXA93MbmV19EdSnhIMD0So+iRGHXQY2rqGyjaWcPWsioK\ndrqkb1vZ3gQwd+tOtlfUfO282IgQ0uMieGj3QioiMnnzndX7jvw1T/o85H0EInJs8hfBgpmw/Hmo\nr4LMMXDOn2HABK8jExERCThbSndz1YPzCQkO4vFrx5LSRQXEWiwoyK2ja8VKl60hPCSY7glRdE84\neNJXW99IYVOy5xvtKyivprB0J+nlm3m8YST3vLPma+fFhoeQ7kvuusVFkBYXQWbXqHbtUaiETiQQ\n1e6G5c/Bgodg21IIjYbhU+C4ayFtqNfRiYiIBKTtFTVMfWg+u2vreeY74+iZGO11SIEneSCsf9/r\nKI5YWEjQgZO+wlz4dwNXTzqfKwadu0/SV1C+7+MV29xIX2pshBI6ETmI4tWwcKabWllT7v7TPO/v\nMGwyRHTxOjoREelkrLXsqKxlXXEl64srWL/d3YeFBDG6ZwLH9erKwPQuhAb7fzGRndV1TJs5n23l\nVTx+7VgGput79agkD4AvnmyTSpee2FPhctDBk75mausbKdv99TV7bUkJnYi/a6iDlf9zo3EbPoag\nUBg00Y3G9RinYiciAcAYEw88CAwBLHANsAp4BugFbAAus9aWehSiyCHV1jeysaTSJW7bK1hfXMm6\nYndfXrW3H1hYcBC9kqKorGngtWUFAESGBjOyRzw5vRLI6dmVkT3iiY0I9eqjHFB1XQPXPbqQVQW7\neGBaDjm9ErwOKXA1VYIsXgk9jvc2ltZQlAtBIZDYr0WHh4UEtfs0XSV0Iv6qfAssegQWPwYVhRDf\nA874PYycCjHJXkcnIkfmXuANa+2lxpgwIAr4FfCutfYuY8ytwK3AL7wMUjo3ay0llbWsK9o70tY0\n8ra5tIqGRrvn2OTYcLKSozl/WDp9kqLJSokhKymGjK6Re/q0bSuvYuGGUhZu2MHCjaX86701NFoI\nMpCd1oXjenVldC83ipceF+nVx6a+oZGbnlzMgg07uGfyCE4boGJixyTZV+myaEXHSOgK8yCpP4T4\nb2EcJXQi/qSxEda/54qcrH4drIX+50DOtdD3DDUEFwlAxpg4YDxwNYC1thaoNcZMBE71HfYo8AFK\n6KQd1NQ3sLFk956ErWmkbX1xBTur6/ccFxYSRJ+kaAZ168IFw7qRlRJNn6QYeidH06UFI2zpcZFc\nODySC4d3A2BXdR1LN5exYEMpizbuYPbCLTw6dyMAGfGR5PTqSo4vweufEktQOzTwbmy0/Pz5L3ln\nRRF3TBzMxBEZbf6eHV5cd7e2388Koxy1ojzo3nYtGFqDEjoRf1BZAksfh4UPQ+lXEJ0MJ/4IRl8N\nXXt6HZ2IHJveQDHwsDFmOLAI+CGQaq3d5jumAEj1KD7pgKy1FFfU+BK1pqTNjbxt3rGbZoNtpHYJ\np09SDBcO70ZWcgx9kqPJSo6hW/ze0bbWEBsRysn9kjm5n5tlUtfQyIptO90o3sYdfLauhDlLt/qO\nDWF0z67k9HRJ3vDMeCLDWveiprWWO19dwQuL8/nJWf2ZOq5Xq75+p9VU6bJohdeRHLvqcijfDDnf\n9jqSQ1JCJ+IVa2HzfFj4EOS+BA010PNEOP03MPAivx7aF5EjEgKMAm621s4zxtyLm165h7XWGmPs\ngU42xtwA3ADQo0ePto5VAkx13d7RtvXbK1lXVME633TJXc1G28JDguidFM2QbnFMHN6NPr7ErXdS\ntGfr2UKDgxiWGc+wzHiuOak31lo276higW+K5sINO/hgVTEAIUGGIRlxbppmzwRyenUlKSb8mN7/\nX++tZeanX/HtE3tx8+l9W+MjSZOUgbD2Xa+jOHZNSWnKYG/jOAwldCLtraYCls120yoLl0FYLIz6\nFuRcA6mDvI5ORFrfFmCLtXae7/lzuISu0BiTbq3dZoxJB4oOdLK1dgYwAyAnJ+eASZ90LqsKdvHi\nknzeyi1gQ0nlPqNtaV0iyEqJZtKIDPokR9MnOYas5Gi6xUW2yxTGY2GM2dMg+hJfyfey3bUs2li6\nJ8F7dO5GHvj4KwB6J0WT07Mrx/VKYHSvrvRJisa0sFDYrM83cvfbq7l4ZAa/PX9Qi8+TFkoeAEuf\ngKpSiOzqdTRHrzDX3fv57zMldCLtpTDPjcZ98QzU7nL94i64B4Z+E8JjvI5ORNqItbbAGLPZGDPA\nWrsKOAPI892mAXf57ud4GKanrLW8s6KI2IgQjuuV0KrT/DqKrWVVvPzFVl5aks/Kgl0EBxlO7JvE\nBcO7keWbItk7KZro8I710y4+KowzBqZyxkA3I7mmvoHl+eUs2FDKwg2lvLOikGcXbQEgMTrMTdP0\nrcUb0i2OsJCvt0uYszSf381ZzpkDU/jLpcP8PtENSMm+SpdFK6HnOG9jORZFKyC8i1sX6Mc61r96\nEX9TXwN5L7tEbtNcCA6HIRe7IieZOWo5INJ53Aw84atwuR74NhAEzDbGXAtsBC7zMD7PNDZabnsl\nl8d8xTESo8M4a1Aq5wxJ44SsRMJDOm8xqPKqOl5fto2XluYz76sdWAsjusdz+0WDOX9Y+jFPOQxE\n4SHBjO6ZwOieCXCKuxiwrriShRt27Cm28lZeoe/YIIZ3j98zijeqR1cWby7lltlfcFyvBP51xaiA\n6I8XkFJ8lS6LVwR4Qpfnpo/6+e81JXQibaF0Iyx6GBbPgt3boWtvOOsOGHkVRKm3jUhnY61dCuQc\nYNcZ7R2LP2lstPzqxWU8vWAz157Um9E9u/LG8gL+9+U2nl6wmdjwEE4fmMKEwWmcMiCZqLCO/7Ol\npr6B91cW8dKSrby3sojahkb6JEXzozP6M3FEN3olRXsdol8xxtA3JYa+KTFMGePWmBbtqmbxxlI3\nirexlBkfref+D9ZhDAQbw4C0WB6clkNEaOe9WNDm4rpDWIwboQtU1ropl4O/4XUkh9Xx/2cUaS+N\nDbDmbTcat+ZtdzVnwHlubVyf01zVJxERAVzvr58/9yUvLMnnptP6csvZ/THGcN7QdGrqG/hsbQlv\nLC/grbwC5izdSnhIEOP7JzNhcBpnDkwlLsq/GlMfi8ZGy7yvdjBnaT6vLdvGzup6kmLCufL4Hkwa\nkcGwzDit8ToCKbERTBiSzoQh6QDsrq1n6eYyFm0oZXtFDTef0a9FbRfkGBjj1tEVB3Cly13boLoM\nUv27IAoooRM5dhVFrvn3okehfBPEpMH4n8HoaRCX6XV0IiJ+p66hkR89s5RXv9zGLWf15+Yz+u2z\nPzwkmNOyUzgtO4U/NgxhwYZS3swt4I3lBbydV0hIkGFcViLnDE7j7MGppMRGePRJjs2KbTt5aWk+\nryzdytbyaqLCgpkwOI2JIzM4MSuREE0HbBVRYSGckJXECVlJXofSuSQPhLVvex3F0SvMc/cp/l0Q\nBTpqQldbCWGakiBtqHY35C9y0yrzXobGOug9Hs6+A7LPh2Bd+RMROZCa+gZuenIJb+cV8qvzsrlh\nfNYhjw8JDmJcViLjshL53QWD+DK/nDeWF/DG8m385qXl/HbOckb36MqEIWmcMziN7glR7fRJjs7W\nsirmLN3KnKWuuElIkGF8/2R+cW42Zw1K7RTTSqWTSB7geuzu3hGYy02KAqPCJbQwoTPGTADuBYKB\nB621d+23/5/Aab6nUUCKtTbet28a8BvfvjuttY+2RuAH1VAPf+8PXTKg+3GQOQa6j4GkAZryJkem\nrto1+S5ZBzvW+e7Xu/tdrvEpEXEw5no3rTKp36FfT0Skk6uua+C7jy/ig1XF3H7RYKad0OuIzg8K\nMozoHs+I7vH8YsIAVhdWuOQut4A7X13Bna+uYHC3LkwYnMaEIWn0TYnxi6mK5bvreG35Nl5a4oqb\nAIzqEc8dEwdz3tB0EjthcRPpBFJ8lS6LV0LPE7yN5WgU5kFst4Bou3DYhM4YEwxMB87C9dJZYIx5\n2Vqb13SMtfbHzY6/GRjpe5wA/B63ENwCi3znlrbqp2iuoQZO+hFsXgArX4Ulj7vt4XGQOdqX4B0H\nGTkQGd9mYUiAqK+Fso37JW3roGQ9lG/G/bX1iUyAxCw3EpeYBUn9od/ZEObfV4NFRPzB7tp6rnt0\nIXPXl/Dni4dy+Zhja5JufMUtBqTF8sMz+7GpZDdv5hbw+vJt3P32au5+ezV9kqP3JHdDM9p3HVp1\nna+4ydJ83l9Z7IqbJEdzy1n9mTgigx6J+u6QDi7ZV+myaEVgJnRFuQExOgctG6EbA6y11q4HMMY8\nDUzE9c85kMtxSRzAOcDb1todvnPfBiYATx1L0IcUFu3WL4GrTlOyDrbMh83zYcsC+OivYBsB32LN\nzOPcCF7mGPcDXaN4HU9DvUvamkbXmiduZZt8fx98IuIgIQt6jIWEK1zilpAFiX0C4gqNiIg/2lVd\nxzWPLGDRxlL+funwPU2jW1OPxCiuH9+H68f3oXBnNW/lFfLm8gL+66tw2C0ugnOGpDFhcBo5bdTr\nrrHR8vlXJcxZspXXlm9jV3U9ybHhTB3Xk0kjMhiS0cUvRgxF2kVcJoTFuhG6QNNQD8WrXVG7ANCS\nhC4D2Nzs+RZg7IEONMb0BHoD7x3i3IwjD/MoGQNJfd1txBVuW80ut/Zp8wKX6K38HyyZ5fZFxLmR\nu+5jXKKXmeO2if9rbHAjas2nRTYlbmUbobF+77FhsS5B6zbKNfVOyNqbuEUl+H2vERGRQFJeVce0\nmfNZll/OvVNGcuHwbm3+nqldIph6fE+mHt+T0spa3l1ZxBvLC3hi3iYe/nQDidFhnD04lXMGp3FC\nVtIBm0+3lLWWFdt2MWdpPnOWbqVgZzXRYcFMGJLOpJHdOCErSY3SpXNqqnRZFICVLnesc7P+AqDC\nJbR+UZQpwHPW2oYjOckYcwNwA0CPHsc2BeOwwmOhz6nuBr5RvLW+ETzfSN4Hd+Gm2hk3XNx9zN5R\nvMS+HWcUr7EBKre7sqyVxe7PIjgEgkIhKMQV9ggKOcjjUAgKbvY4xD1vy2SosdGtXWuerDU9Lt0A\nDbV7jw2Ncgla2hAYNLHZSFsWRCcraRMRaQellbVMnTmPVQW7uP/KUZwzOK3dY+gaHcalozO5dHQm\nlTX1fLCqmDdyC3h56Vaemu963Z0xMIUJQ9IY37/lve7yy6qYszSfl5bks7qwgpAgw6kDkvn1+QM5\nc2AqkWHqcSZCSjasfsvrKI5coa8gSgBUuISWJXT5QPdmzzN92w5kCnDjfueeut+5H+x/krV2BjAD\nICcnx+6/v00Z44pZJPWDkVe6bdXl+47i5b0Ei321XCLim03TPA4yRkNEl3YN+bAaG2F3iUvUKgrd\n/a6Cr99XFMGR5d6Hd1TJ4H6J4T5JZQhUlblRtx3rob5673uFREBCHzdVdsC5+460xaYpaRMR8VDx\nrhqmPjSP9dsrmTE1h9OyU7wOiejwEM4fls75w9Kprmvgs3Xb97RCeGnpViJCgzilfzIThqRxenYq\ncZH7Viwu213La8sKeGlJPvM3uOImOT27csekIZw/NJ2E6DAvPpaI/0rOdvUsAq3SZVEemGD3GzMA\ntCShWwD0M8b0xiVoU4Ar9j/IGJMNdAXmNtv8JvAnY0zT4qOzgV8eU8Qt8MdX8wgLCSIiJJiI0GAi\nQoMID/U9DgnybXPb3bZgwkOD9tyHh3fBZJ0OWae7F2xshJI1zUbxFsDad9gzipcyaN+Kmol92yaZ\nsBaqSn0J2f5JWrNbRcG+UwybRCVCbLpLdlIHu35psWluW0wKmCBoqHPnNta5+cN7Hte5Eb09j+v3\n3po/b6hzxzQ2NHtc73utAz323ddXH/x1wqJdkpZ1ukvgErPcn3Fst44zWioi0oEUlFdz5YOfk19W\nxcNXH8eJff2v/1dEaDCnZ6dyenYq9Q2NzN+wgzeXF/BmbiFv5u7tdTdhSBrxkWHMWZrP+6uKqGuw\n9E2J4adnu+Im/t4mQcRTyb5Kl0UroNeJ3sZyJArz3O/N0MDocXnYhM5aW2+MuQmXnAUDM621ucaY\nPwALrbUv+w6dAjxtrbXNzt1hjLkDlxQC/KGpQEpbqW9o5Il5m6iua6DxKMf6jIHwkL3JXlPiFx7a\nh4iQvkRETiW+124GNKymb00evavy6L70OSIWPQJAdWgcJfHDKEsYwa7kUVSnDCc0Ko6I0CCiw0NI\niAojPips75x9a10n+j0J2oFG1XyJWvNphU0iu/qSslR3JaEpSdtzn+r2hagssoiItK38siqueOBz\ntu+q4bFrxjKmt/9flQ8JDtrTePr3Fw7miy1lvJFbwJvLC/j1i8sBSIkNZ9q4XkwamcHgbipuItIi\nKb5Kl8UBltAV5UL6CK+jaDHTLP/yCzk5OXbhwoXH/DrWWuoaLNX1DdTUNVJd10BNfQPVvsd77vfZ\n1kBN/d7H1XvOazrW97xpX/3e42rq6shs2MKooDWMMmsYFbSG/kFuZmqDNay23Vnc2I8tNplkU0aK\nKSU9uJw0U0qSLSWcrydqdaGx1EenQkwawXHdCI3vholN2zdhi0kLmKsHIiL7M8YsstbmeB1HoGit\n78i2srGkkisemMfO6joevWYMo3oEdnVgay2rCnexs6qe0T27qriJyJGyFu7qAcMmw/l/9zqalqmp\ngD9nwGm/hlN+7lkYR/L92NpFUfyGMYawEONGwdop37HWNksIG9lUsQOzdSGhWxeRXrCIydvnE1K3\ni9rgaCrCkigPTmSbyWSpjSe/IZ5NtV1YXxPLlv9v787joyzvvY9/ftlD9oQJO7InbAISEGURrVVU\nRFsXrEtrq9jWDfU8eux5Tk972p4e+3RTXKq4191q607RVhZBpIIgkLAjQlgyCXuAkO16/pgJRAVJ\nwkzumcz3/XrlNZk7MwgR0poAACAASURBVHN/My/IlV+u+/pdddn4XTZVVcmw78jrJ8YbOe2SyE1L\nIjcNctN2kptWSW5aEnlpSeSkNXwt8JHTLonEeF2SKCIi4be+vJKrHl1IVW0dL0wZxaAu0d8l2swo\n7Bhh6+RFoklDp8to2rqgIWuUNESBNlzQecHMDq/PAyCrM3SZBEwK3K+vh5oDJCWnkwvkEtjjoTHn\nHPur69i1v5od+6uPfXugmuKte9m5v5o9B2uOmSkzJeELRV5usPDLCxZ8eenB27Rk8tKTSEvWPwkR\nEWmeNWX7uPLRhTjnePGGUSqCROQIXyGs+bvXKZquocNllGwqDiroWldcHCSnf+1DzIz05ATSkxOa\nvNC6pq6e3Qdq2Lm/+sjHgWp2VgYKv4ZCcMvuKlZsCRSB1XX1R32t1MR4fBnJtE9Pon16Mu0zkmmf\nnozvy/czkklLitcaAhGRGFe8dQ/XPP4vEuKM528YRZ/8DK8jiUgk8RUG9nzevwPS8rxOc3z+EkhM\ng+weXidpMhV0bUBifBy+jECR1RQNs4A7KwOF36791VRUHmLH/moq9h2iovIQ5ZWH+HzHARZ/voud\nB6o52lLLlMS4QJGX3lDkJX3hfvv0pEBxmJFMRnKCij8RkTbm0827uebxhaQnJ/D8lFH0aJ/mdSQR\niTSNG6OkjfE2S1OUFQcyR1EndRV0MajxLGD3vOPPAtbW1bPzQDUV+wKFX0XlIcqDhV9FZeBY6a4D\nLN28m537Dx21u2hSQhy+9EYzf+nJtG9UAPoOzwQmk5mq4k9EJNIt2riTa5/8mJy0RJ6/fpTa94vI\n0X1h64IIL+icC8zQFZzvdZJmUUEnx5UQH0d+Rgr5GcfvLlNX79h1oLpRwXfocCFYHiwAt+2pYtmW\nPezcX03dUaq/pPg48g4Xfkn0yU9nfEE+RT1ySE6ID8e3KCIizfDh+gquf3oRHTNTeG7KqXTKSvU6\nkohEqszOkJwZHY1RKv1wYEdgv+YoooJOQio+zg7Puh1PfbD4a5jlOzLzd6Qg9O87xPx1O3j0g89o\nlxTP6D7tGV/gY3xBPl2y9QuEiEhrm7OmnBv+vIjuue14bsqpTfpjn4jEMLPAOjp/FBR0/mBDlCjq\ncAkq6MRDcXFGXnoyeenJFHDsRfT7D9WyYP0OZq/xM2tVOe+VlAHQr0M6Zxbkc0aBj6KTco9s1C4i\nImHxj5IybnzuE/rkp/PMdSPJa8If70REyC+EVe94neL4ykoCt5qhEwmttOQEzh7QgbMHdMA5x/ry\nSmatKmf2Gj9PzP+MR+ZuID05gdF98hhfkM/4Ap8u/xERCbEZy7dxywtLGNg5k6d/MJLsdkleRxKR\naOErhE/+DPsrIK2912mOzV8CafmRnfEoVNBJVDEz+uRn0Cc/gynjelF5qJYP11Uwe005s1f5mVkc\nmL0r7JhxuLgbflKONlgXETkBry/dwh0vf8rQbtk8+f0RZKYkeh1JRKKJL9jp0r8Seo71NsvXKSuO\nqv3nGqigk6iWnpzAOQM7cs7AjjjnWOuvZNYqP7NXl/PYBxt4eM56MpITGNP3yNq7Dpla7yEi0lQv\nf7yZf//rMk7tmcvj3xtBWrJ+dRCRZsoPdrosXxW5BV19XSBf0XVeJ2k2/VSWNsPM6Nchg34dMvjh\nGb3ZV1XD/HU7mBNcezdjxXYA+nfK5MxgcXdK92wSNHsnInJUz3z0OT99bQVj+7Zn+jVFpCap07CI\ntEBGJ0jOCszQRapdG6G2SjN0IpEkIyWRCYM6MmFQYPZuddm+wNq71X4embuBh2avJyMlgXF9fZxR\n4GN8Px/5mr0TEQHgsQ828Ku3V3J2/3weuPIUUhJVzIlIC5kFGqNE8tYFZdHZ4RJU0EmMMDMKO2ZS\n2DGTH4/vzd6qGuavrWD26nJmrfbz9vJtAAzsnMmZwbV3Q7tp9k5EYtODs9bx25mrOW9QR+67Ypi6\nCIvIifMVwqq3vE5xbP4SwI6s94siKugkJmWmJHLe4E6cN7gTzjlWbtvHrNV+5qwu509z1vPArHVk\npSYytm97xhfkc0Y/H74MtecWkbbNOccf/7GWaf9cy0VDO/P7y4boD1siEhq+Qvjkaagsh3Sf12m+\nqqwYcntBUjuvkzSbCjqJeWbGgM6ZDOicyU1n9mHPwRrmra0IFHhrynlrWWD2bnCXLM4s8HFGQT5D\nu2UTH2ceJxcRCR3nHL/5+2oenrOey4Z35Z5LTtbPOREJnfzgzFf5ysgs6PwlUbl+DlTQiXxFVmoi\nF5zciQtO7kR9vaNk215mrw50znxg1jqmvb+O7HaJnFmQz7+d04+uOdH3lxwRkcacc/z3myU89eFG\nrh7VnV9MGkScijkRCSVfsNOlfxX0HOdtli+rOQg7N8CgS71O0iIq6ES+RlycMahLFoO6ZHHzWX3Z\nfaCaD4KzdzNXbOcfK8v4328PZuLJnb2OKiLSIvX1jv98fQXPL9zED0b35KcT+2OmYk5EQiyjI6Rk\nBWboIk35KnD1UTtDpwvjRZohu10SFw7pzB8uH8qMqePo7Uvn5ueXcOdfPmX/oVqv44mINEtdveOu\nV5fx/MJN3Di+t4o5EQkfs8AsnT8CO12WlQRu8wd6m6OFVNCJtFD3vHb85UencctZfXjlk1Im3j+P\nZaW7vY4lItIkNXX13PbSUl5ZXMrtZ/fjznMLVMyJSHjlFwZm6JzzOskX+UsgIRVye3qdpEVU0Imc\ngMT4OP7tnAJenDKKqpo6vv3Qhzw8Zz319RH2g0pEpJHq2npufv4T3vx0K/8+oZCpZ/dVMSci4ecr\nhIO7YH+510m+qKwYfAUQF537baqgEwmBU3vl8fep4zhnYAfumbGKqx9fyPY9VV7HEhH5iqqaOn70\n7GJmFpfxXxMH8OPxvb2OJCKxomGPN3+EraPzl0CH6LzcElTQiYRMVrtEHrzyFH5zyWCWbNrNeffN\n5d3i7V7HEhE57GB1HVP+vIj3V/n5n28N4gdjovPyIhGJUvnBTpflEbSObv8OqCyD/OhsiAIq6ERC\nysyYPKI7b906hi45qdzwzGL+79+Wc7C6zutoIiLc//5a5q+r4LeXnsxVp57kdRwRiTXpHSAlO7Jm\n6PzFgdso7XAJ2rZAJCx6+9L5649H8/t3V/PI3A0s/Gwn064YxoDOmV5HE5EYdstZfRnVK49x/SJw\nU18RafvMArN0kTRDd7jDZfQWdJqhEwmTpIQ4fnJ+f5697lT2Hqzh4gfn88S8z3CR1tlJRGJGalK8\nijkR8ZavMDBDFym/D/mLITU3MHsYpVTQiYTZmL7tmTF1LOP6tecXb5Vw7ZMfU77vkNexRERERFqf\nrxCqdkOl3+skAWXBhihR3OlXBZ1IK8hLT+bR7xbxy4sH8dGGHZx331xmrY6QH2QiIiIirSU/2Omy\nPALW0dXXB2YLo/hyS1BBJ9JqzIxrRp3Em7eMoX16Mt9/8mP++81iqmrUMEVERERihC/Y6dIfAevo\ndn8ONfujuiEKqKATaXX9OmTw2k2jufb0Hjw5fyMXPziftWX7vI4lIiIiEn7p+ZCaExkzdP6GhijR\nuwcdqKAT8URKYjw/nzSQJ64tonzfISbeP49nP/pcDVNERESkbTMLzNJFwgzd4Q6Xhd7mOEEq6EQ8\ndFZhB2bcNpZTe+Xxn6+t4IZnFrNzf7XXsURERETCJ78wMEPn9R+y/cWQfRIkZ3ib4wSpoBPxWH5G\nCk9dO4KfThzAnNXlTLh3LvPXVXgdS0RCyMw2mtlyM1tqZouCx3LN7D0zWxu8zfE6p4hIq/AVQtUe\nqCzzNkdDh8sop4JOJALExRnXjenJ3246nYyUBK5+fCH/O2Ml1bX1XkcTkdA50zk31DlXFLx/N/BP\n51xf4J/B+yIibZ8veImj38N1dLWHYMe6qO9wCSroRCLKwM5ZvHXLWL4zsjuPzNnAJX/6kA3llV7H\nEpHwuAh4Ovj508DFHmYREWk9+cFOl+UerqMrXw2uLuo7XIIKOpGIk5oUz6+/NZiHrx7O5l0HmHj/\nPF7+eLMapohENwe8a2aLzeyG4LEOzrltwc+3Ax28iSYi0srSfJCa6+0MXRvpcAkq6EQi1oRBHZkx\ndSxDumZz16vLuPmFJew5UON1LBFpmTHOuVOA84CbzGxc4y+6wF9sjvpXGzO7wcwWmdmi8vLyVogq\nIhJmZoFZOi9n6MqKIT4J8np7lyFEVNCJRLBOWak8e/2p3DWhgJkrtnP+tA/412c7vY4lIs3knNsS\nvPUDfwNGAmVm1gkgeOs/xnOnO+eKnHNFPp+vtSKLiISXrzCwdYFXVyD5V0L7AohP9Ob8IaSCTiTC\nxccZN47vw6s/Pp3EeOOK6Qv4w7urqa1TwxSRaGBmaWaW0fA5cA6wAngD+F7wYd8DXvcmoYiIB3yF\ncGgP7Nvuzfn9JW1i/RyooBOJGkO6ZfPWrWP59ildmfb+Oi5/ZAGbdx7wOpaIHF8HYJ6ZfQr8C3jb\nOfd34B7gm2a2Fjg7eF9EJDY0bOZd7sE6uoO7YO+WNtHhElTQiUSV9OQEfnfZEKZ9Zxhr/ZWcf98H\nvL50i9exRORrOOc2OOeGBD8GOuf+J3h8h3PuG865vs65s51zup5aRGKHL9jp0u/BOrqGZixtYA86\naGJBZ2YTzGy1ma0zs6Puk2Nml5tZiZkVm9nzjY7XBTdSXWpmb4QquEgsmzSkM+/cOpaCjhlMfXEp\nd7y0lH1VapgiIiIiUSLdB+3yvJmhKysO3LaRGbqE4z3AzOKBB4FvAqXAx2b2hnOupNFj+gI/AUY7\n53aZWX6jlzjonBsa4twiMa9bbjtevGEUD8xax7R/rmXR57u474qhDOue43U0ERERkePz9fdohq4E\nUrIgs3PrnzsMmjJDNxJYF7xkpBp4kcBmqI1NAR50zu2Cw128RCTMEuLjuO3sfrz8w9Ooq3dc+vAC\nHnh/LXX12rNOREREIlx+YWDrgtbudFlWEth/zqx1zxsmTSnougCbG90vDR5rrB/Qz8zmm9lHZjah\n0ddSgvvnfGRmF59gXhE5iqIeubwzdSznD+7E795dw3emf6SGKSIiIhLZfIVwaC/s29Z653QusIau\njXS4hNA1RUkA+gLjge8Aj5pZdvBrJznnioArgXvN7Cu792nTVJETl5WayLQrhvL7y4awctteJtw7\nl5cXbcZ5tb+LiIiIyNfxBTtd+ltxHd2e0sB2CW1k/Rw0raDbAnRrdL9r8FhjpcAbzrka59xnwBoC\nBV7jzVQ3ALOBYV8+gTZNFQkNM+OS4V2ZcdtYBnXJ4q5XlnHDM4upqDzkdTQRERGRL8oPdrosb8V1\ndP5gG5A20uESmlbQfQz0NbOeZpYEXEFgM9TGXiMwO4eZtSdwCeYGM8sxs+RGx0cDJYhIWHXNaccL\nU0bxf8/vz5zV5Uy4dy7/KCnzOpaIiIjIEWntoV371p2hO9zhsn/rnTPMjlvQOedqgZuBmcBK4GXn\nXLGZ/cLMJgUfNhPYYWYlwCzgTufcDqA/sCi4meos4J7G3TFFJHzi4owp43rxxi2j8WWkcP2fF3H3\nq8uoPFTrdTQRERGRgPz+rT9Dl9Ut0OWyjTjutgUAzrl3gHe+dOy/Gn3ugDuCH40f8yEw+MRjikhL\nFXbM5LWbTueP763lkbnr+XD9Dv44eQjDT8r1OpqIiIjEOl8hLHsp0KykNbpOlpW0qfVzELqmKCIS\nwZIT4rn7vEJeuuE06p3jsocX8NuZq6iurfc6moiIiMQyX0Gg0+XereE/V10NVKxpUx0uQQWdSEwZ\n2TOXv982jsuGd+PBWev51kPzWVO2z+tYIiIiEqsON0ZphXV0FWuhviawB10booJOJMakJyfwm0tP\nZvo1w9m+p4qJ98/j8XmfUa/NyEVERKS1+YIFnb8V1tE1dLhsQw1RQAWdSMw6Z2BHZt4+jnF9ffzy\nrRKufnwhW3Yf9DqWiIiIxJK0PEjztc4MXVkxxCVA+37hP1crUkEnEsPapyfz6HeH85tLBvPp5t1M\nuHcuf1tSqs3IRUREpPX4Cltvhi6vLyQkhf9crUgFnUiMMzMmj+jOjKnjKOiQwe0vfcrNzy9h1/5q\nr6OJiIhILMjvD+WrA50uw6mspM01RAEVdCIS1D2vHS/98DTumlDAuyXbOffeucxe7fc6loiIiLR1\nvgKo3gd7t4TvHFV7Yc+mNrdlAaigE5FG4uOMG8f34bWbRpPdLpFrn/yYn762ggPV2oxcREREwqQ1\nGqP4g2v0OrStDpeggk5EjmJg5yzeuHkM14/pybMLP+eCafNYsmmX17FERESkLWqNrQv8xcFzaYZO\nRGJESmI8/zlxAM9dfyqHauq49OEF/PG9NdTUaTNyERERCaF2uZCWH94ZurISSMqA7O7hO4dHVNCJ\nyNc6vXd7Ztw2jouGdOa+f67lkj99yPrySq9jiYiISFuSXxjmGbqSwEygWfjO4REVdCJyXFmpifxh\n8lAeuuoUNu08wAXTPuDpDzdqewMREREJDV8YO106F9iDrg12uAQVdCLSDOcP7sS7t43j1J55/OyN\nYr77xL/YvqfK61giIiIS7XwFUF0Je0pD/9r7tkHVbshvew1RQAWdiDRTfmYKT31/BL+6eBCLNu7i\n3Hvn8uanW72OJSIiItHscGOUMKyjKysJ3GqGTkQkwMy4etRJvH3rGHq0T+OWF5Yw9cUl7DlQ43U0\nERERiUa+wsCtPwzr6PzBgq4NdrgEFXQicgJ6+dJ59Uenccc3+/HWsm2ce+9c5q2t8DqWiIiIRJt2\nuZDeITwzdP4SyOgUOEcbpIJORE5IQnwct36jL3+78XTaJcdz9eML+e83i6mqqfM6moiIiEQTX2F4\nZujKitvs7ByooBOREDm5azZv3zKWa0/vwZPzNzLx/nksL93jdSwRERGJFvnBTpf1Idzztq428Jpt\ndP0cqKATkRBKTYrn55MG8sx1I9lXVcO3HprPA++vpVabkYuIiMjx+AqgZj/sDWGny50boO5Qm+1w\nCSroRCQMxvb1MfO2cZw3uBO/e3cNlz2ygI0V+72OJSIiIpHMF+x06Q/hOjp/ceBWM3QiIs2T3S6J\n+78zjPuuGMp6fyXn3fcBzy/cpM3IRURE5Ojyg50uy0O4jq6sBCwe2heE7jUjjAo6EQmri4Z2Yebt\n4xh+Ug7/8bfl3PriUvYfqvU6loiIiESa1BxI7xjiGboSyOsNiSmhe80Io4JORMKuU1Yqf/7BSO48\nt4C3l23logfns86/z+tYIiIiEmnyC0M8Q9e2O1yCCjoRaSVxccZNZ/bhmetOZdf+aiY9MJ83P93q\ndSwRERGJJL4Qdrqs3g+7NkKHttsQBVTQiUgrG92nPW/fOpb+nTK55YUl/PyNYqpr1QVTRERECHa6\nPAB7Np/4a/lXAU4zdCIiodYxK4UXbxjFD0b35KkPN3LF9AVs23PQ61giIiLitfxgp8vyEKyji4EO\nl6CCTkQ8khgfx39dOIAHrhzG6u37uGDaPOatrfA6loiIiHjJF+x06Q/BOrqyEkhMg+weJ/5aEUwF\nnYh4auLJnXn95jHkpSVxzRMLeeD9tdTXa2sDERGRmJSaDRmdQjdDl18IcW275Gnb352IRIU++em8\ndtNoJg3pzO/eXcP1f17E7gPVXscSERERL/gKQzdD18bXz4EKOhGJEGnJCdw7eSi/vGggH6wtZ+L9\n81heusfrWCIiItLa8vtDxZoT63RZ6YcDFW2+wyWooBORCGJmXHNaD17+4WnU1zsu+dOHPL9wE87p\nEkwREZGY0dDpcvfnLX+NsmBDlIYmK22YCjoRiTjDuufw1q1jObVXLv/xt+X8n78s42B1ndexRERE\npDX4Gjpdrm75a/hLArf5mqETEfFEbloST31/JFO/0Ze/LinlWw/N57OK/V7HEhERkXDzFQRuy09g\nHV1ZCaT5IN0XmkwRTAWdiESs+Djj9m/248lrR7B9bxWT7p/H31ds9zqWiIiIhFNqNmR0Dm4M3kL+\n4phoiAIq6EQkCowvyOetW8bQy5fGj55dzK/fWUlt3QkslBYREZHIll/Y8hm6+rpAMRgDDVFABZ2I\nRImuOe14+Uencc2ok5g+dwNXPrYQ/94qr2OJNJmZxZvZEjN7K3i/p5ktNLN1ZvaSmSV5nVFEJGL4\n+kN5Cztd7toItQc1QyciEmmSE+L55cWDuHfyUJaX7uH8afP4aMMOr2OJNNVUoPGfm38D/NE51wfY\nBVznSSoRkUjkKwgUZbs3Nv+5DR0uO6igExGJSBcP68JrN40mMyWBqx5byCNz1mtrA4loZtYVuAB4\nLHjfgLOAV4IPeRq42Jt0IiIRKP8EOl36SwA70i2zjVNBJyJRqaBjBq/fPJpzB3bgf2es4ofPLGZv\nVY3XsUSO5V7gLqDh2qE8YLdzrjZ4vxTo4kUwEZGI1NDp0t+CdXRlxZDbE5LahTZThFJBJyJRKyMl\nkQevPIWfThzA+6v8TLp/HiVb93odS+QLzGwi4HfOLW7h828ws0Vmtqi8vDzE6UREIlRKFmR2gfIW\ndLr0l8TM+jlQQSciUc7MuG5MT168YRQHa+r41kPz+cuizV7HEmlsNDDJzDYCLxK41PI+INvMEoKP\n6QpsOdqTnXPTnXNFzrkin6/t76ckInKYr7D5M3Q1B2HnhpjpcAlNLOjMbIKZrQ524rr7GI+53MxK\nzKzYzJ5vdPx7ZrY2+PG9UAUXEWmsqEcub90yllO653DnK8v4yV+XUVVT53UsEZxzP3HOdXXO9QCu\nAN53zl0FzAIuDT7se8DrHkUUEYlM+f2hYk1gG4KmKl8Nrl4zdI2ZWTzwIHAeMAD4jpkN+NJj+gI/\nAUY75wYCtwWP5wI/A04FRgI/M7OckH4HIiJBvoxknrluJDeO780L/9rMpQ9/yOadB7yOJXIs/w7c\nYWbrCKype9zjPCIikcVXALVVgW0ImspfErjVDN0XjATWOec2OOeqCVwuctGXHjMFeNA5twvAOecP\nHj8XeM85tzP4tfeACaGJLiLyVQnxcdw1oZDHvlvE5zsOcMG0D3h/VZnXsUQAcM7Nds5NDH6+wTk3\n0jnXxzl3mXPukNf5REQiiq8FnS7LiiEhBXJ7hSdTBGpKQdcFaLwg5WiduPoB/cxsvpl9ZGYTmvFc\nEZGQO3tAB96+ZSzdctvxg6cW8duZq6ir19YGIiIiUaOh02V5M9bR+UsCz4uLD0+mCBSqpigJQF9g\nPPAd4FEzy27qk9XBS0TCoXteO1798elMLurGg7PW890nFlJRqUkQERGRqJCSCZldwd+MTpdlJZAf\nO5dbQtMKui1At0b3j9aJqxR4wzlX45z7DFhDoMBrynPVwUtEwiYlMZ7fXHoy/+/Sk1m0cRcTp81j\n8ec7vY4lIiIiTZFf2PQZugM7oXI7dIidhijQtILuY6CvmfU0syQCHbre+NJjXiMwO4eZtSdwCeYG\nYCZwjpnlBJuhnBM8JiLSqi4v6sZfbzydpIQ4Jj/yEU/M+wzndAmmiIhIRPMVQsXapnW6LCsO3MZQ\nh0toQkHnnKsFbiZQiK0EXnbOFZvZL8xsUvBhM4EdZlZCoA3znc65Hc65ncAvCRSFHwO/CB4TEWl1\nAztn8eYtYxhfkM8v3irh5heWUHmo1utYIiIiciy+wqZ3uozBDpcQWPt2XM65d4B3vnTsvxp97oA7\ngh9ffu4TwBMnFlNEJDSyUhOZfs1wHpm7gd/OXMXKbXt5+Orh9OuQ4XU0ERER+bL8hk6XqyCv99c/\ntqwYUnMhvUP4c0WQUDVFERGJGnFxxo/H9+a560ex92AtFz0wn6fmf0Z1bb3X0URERKSxhk6X/ias\no/OXBGbnzMKbKcKooBORmHVa7zzevnUMw0/K4edvlnDOH+cwY/k2ra0TERGJFMkZkNUtMEP3derr\nA0VfjK2fAxV0IhLjOmSm8Mx1I3ni2iIS4+P48XOfcMmfPmTRRi33FRERiQi+wuNvXbBnE1RXxlyH\nS1BBJyKCmXFWYQdmTB3LPd8eTOmug1z68AJ++MwiNpRXeh1PREQktuUXQsWar+90WRZsiBJje9CB\nCjoRkcMS4uO4YmR3Zt85nju+2Y95ayv45h/n8tPXVmhDchEREa/4CqHuEOz87NiP8TdsWVDYOpki\niEXaWpGioiK3aNEir2OIiFC+7xD3/XMNL/xrMykJcfzojN5cP7YXqUnxXkdrM8xssXOuyOsc0UJj\npIjEipqaGkpLS6mqqoLa6sCG4Wk+SEw9+hP2V0BdNWR2bt2gJyglJYWuXbuSmJj4hePNGR+btG2B\niEgs8mUk86uLB/P90T35zYxV/P69NTy78HPu+GY/Lh3ejfi42OqiJSIi0lpKS0vJyMigR48emKuH\n7fWQ0QkyOh79Cf6VEJ8Meb1aN+gJcM6xY8cOSktL6dmzZ4tfR5dciogcR29fOtO/W8RffnQanbNT\n+fdXl3PefXOZtcqvjpgiIiJhUFVVRV5eHmYGcfEQnwQ1VUd/sKsPbD6emNK6IU+QmZGXlxeYhTwB\nKuhERJpoRI9c/vrj03noqlM4VFvP95/6mKseW8jy0j1eRxMREWlzrPF+cgkpgaLtaBqOJ0RXQQdf\n+h5bSAWdiEgzmBnnD+7Ee7efwc8vHMCq7fu48IF5TH1xCZt3HvA6noiISNvUUNAd7cqYhpm7Y62v\na4Hdu3fz0EMPNft5559/Prt37w5ZjqZQQSci0gJJCXFcO7ons+8cz43je/P3Fdv5xu/n8Ot3VrLn\nQI3X8URERNqWxBTAQe1Ruk7XHgQMEpJDdrpjFXS1tbVf+7x33nmH7OzskOVoChV0IiInIDMlkbsm\nFDL7zvFMGtqZRz/YwLjfzuLRuRs4VPs1++WIiIhI0zVcTnm0yy5rqgJft9CVNnfffTfr169n6NCh\njBgxgrFjxzJp0iQGDAhsXH7xxRczfPhwBg4cyPTp0w8/r0ePHlRUVLBx40b69+/PlClTGDhwIOec\ncw4HDx4MWb7GzJwYzgAADP1JREFU1OVSRCQEOmWl8rvLhnDdmJ7874xV/M87K3l6wUbuPLeAC0/u\nTJw6YoqIiLTIf79ZTMnWPVC9H+L3BhqkNFa9P9A4JWFXk19zQOdMfnbhsTchv+eee1ixYgVLly5l\n9uzZXHDBBaxYseJwN8onnniC3NxcDh48yIgRI7jkkkvIy8v7wmusXbuWF154gUcffZTLL7+cV199\nlauvvrrp33gTaYZORCSE+nfK5M8/GMkz140kMyWRqS8u5aIH5/Ph+gqvo4mIiEQxA7NAR8svcIGP\nEM7OHc3IkSO/sLXAtGnTGDJkCKNGjWLz5s2sXbv2K8/p2bMnQ4cOBWD48OFs3LgxLNk0QyciEgZj\n+/oYfUt7Xlu6hd/NXM2Vjy7krMJ87j6vkH4dMryOJyIiEjUOz6TtWB/YPDy//5EvHqqEHWshtxek\nZIUtQ1pa2uHPZ8+ezT/+8Q8WLFhAu3btGD9+/FG3HkhOPrKmLz4+PmyXXGqGTkQkTOLijG+f0pX3\n/8947j6vkI837mTCvXO5+9VllO09sT1nREREYk5iSqApSuNOl7XBIikhdB0uATIyMti3b99Rv7Zn\nzx5ycnJo164dq1at4qOPPgrpuZtLM3QiImGWkhjPj87ozeSibtz//jqe+Wgjry/dypSxPbnhjN6k\nJ+tHsYiIyHElNOp02bCJeE0VWDzEJ4b0VHl5eYwePZpBgwaRmppKhw4dDn9twoQJPPzww/Tv35+C\nggJGjRoV0nM3l7mj7eXgoaKiIrdo0SKvY4iIhM2mHQf4fzNX8daybbRPT2Lq2f24YkQ3EuNj76IJ\nM1vsnCvyOke00BgpIrFi5cqV9O/f/4sHq/dDxRrI6Qmpwa0BKtYEbtv3a92AIXS077U542Ps/fYg\nIuKx7nnteODKU3jtptH08qXz09dWcO69c5lZvJ1I+yObiIhIxPjy1gXOBbcsCO3lltFGBZ2IiEeG\ndsvmpRtG8dh3izDgh88s5vJHFvDJpqa3XRYREYkZcfGBLQtqguvm6mrA1R25/DJGqaATEfGQmXH2\ngA7MvG0cv/7WYD6rOMC3H/qQG59bzMaK/V7HExERiSwJKUdm6MLUECXaaCW+iEgESIiP48pTu3PR\n0M48+sEGps/dwLvFZVw8rAujeuUxtFs2vdqnaYNyERGJbYkpcGhfYD+6mqojx2KYCjoRkQiSlpzA\nbWf348qR3bn3n2t5c+lWXllcCkBmSgJDumUzrHsOw7plM7RbNjlpSR4nFhERaUWHO11WB2bo4hIh\nLrZLmtj+7kVEIlR+Zgq//tZgfnXRINaXV7Jk826WbNrN0s27eeD9tdQHe6f0yGvHsO45DO2WzbDu\n2RR2zCQpQVfTi4hIG9VweWVtVWCGLjG2L7cEFXQiIhEtLs7o2yGDvh0yuLyoGwD7D9WyfMueYIG3\ni/nrKvjbki0AJCXEMbhL1uECb1j3HDpnpWCmSzVFRKQNSEgO3NYcDBR1KRne5glKT0+nsrLSk3Or\noBMRiTJpyQmM6pXHqF55ADjn2LaniiWbdrNk0y6Wbt7Nsx99zuPzPgPAl5EcuESzezbDuuVwctcs\n0rSZuYiIRKOGTpdVuwEX8w1RQAWdiEjUMzM6Z6fSOTuVC07uBEBNXT2rtu1jyeZdLN20myWbd/Nu\nSRkAcQb9OmQwrHt2cCYvhz6+dDVcERGR6JCQAof2Bj4P0yWXd999N926deOmm24C4Oc//zkJCQnM\nmjWLXbt2UVNTw69+9SsuuuiisJy/OVTQiYi0QYnxcQzumsXgrll897TAsV37q1lauvtwgff2sm28\n8K/NAGQkJ3BytyyGdQusxxvaPZv26ckefgciIiJBM+6G7cuP3K87BHXVgEFSWuC2uToOhvPuOeaX\nJ0+ezG233Xa4oHv55ZeZOXMmt956K5mZmVRUVDBq1CgmTZrk+bIGFXQiIjEiJy2JMwvyObMgH4D6\nesdnO/YHC7zApZp/mrOeumDHlW65qYcLvGHdsxnQOZPkhHgvvwURERGwYPMvM1pUzDXBsGHD8Pv9\nbN26lfLycnJycujYsSO33347c+fOJS4uji1btlBWVkbHjh3DkqGpVNCJiMSouDijty+d3r50Lhne\nFYCD1XWs2Lrn8Fq8jzfu5I1PtwKQFB/HgM6Zhwu803rnkZ8R23v/iIhIK/jyTFr1AahYDSk5kNsj\nbKe97LLLeOWVV9i+fTuTJ0/mueeeo7y8nMWLF5OYmEiPHj2oqqoK2/mbSgWdiIgclpoUz4geuYzo\nkXv42PY9VSzdvOvw1gkvfbyZpz7cyB8uH8K3T+nqYVoREYlJCSlg8cHLLcNn8uTJTJkyhYqKCubM\nmcPLL79Mfn4+iYmJzJo1i88//zys528qFXQiIvK1OmalMCGrExMGBRqu1NbVs7psH12y1VlMREQ8\nEBcH+QMCHS/DaODAgezbt48uXbrQqVMnrrrqKi688EIGDx5MUVERhYWFYT1/U6mgExGRZkmIj2Ng\n5yyvY4iISCyLb50yZvnyI81Y2rdvz4IFC476OK/2oAOI8+zMIiIiIiIickJU0ImIiIiIiEQpFXQi\nIiIiIiJRSgWdiIiIiIhEHOec1xHCLhTfowo6ERERERGJKCkpKezYsaNNF3XOOXbs2EFKyont6aou\nlyIiImFkZinAXCCZwLj7inPuZ2bWE3gRyAMWA9c456q9SyoiEjm6du1KaWkp5eXlXkcJq5SUFLp2\nPbE9XVXQiYiIhNch4CznXKWZJQLzzGwGcAfwR+fci2b2MHAd8Ccvg4qIRIrExER69uzpdYyooEsu\nRUREwsgFNGxQlBj8cMBZwCvB408DF3sQT0REopwKOhERkTAzs3gzWwr4gfeA9cBu51xt8CGlQBev\n8omISPRSQSciIhJmzrk659xQoCswEihs6nPN7AYzW2Rmi9r6WhIREWm+iFtDt3jx4goz+zwEL9Ue\nqAjB68QSvWfNo/er+fSeNV9bf89O8jpAa3LO7TazWcBpQLaZJQRn6boCW47xnOnAdAAzKw/BGNnW\n/02Fg96z5tN71nx6z5qvLb9nTR4fI66gc875QvE6ZrbIOVcUiteKFXrPmkfvV/PpPWs+vWfRz8x8\nQE2wmEsFvgn8BpgFXEqg0+X3gNeP91qhGCP1b6r59J41n96z5tN71nx6zwIirqATERFpYzoBT5tZ\nPIGlDi87594ysxLgRTP7FbAEeNzLkCIiEp1U0ImIiISRc24ZMOwoxzcQWE8nIiLSYm25Kcp0rwNE\nIb1nzaP3q/n0njWf3jMJNf2baj69Z82n96z59J41n94zwJxzXmcQERERERGRFmjLM3QiIiIiIiJt\nWpsr6MxsgpmtNrN1Zna313kinZl1M7NZZlZiZsVmNtXrTNEiuFHwEjN7y+ss0cDMss3sFTNbZWYr\nzew0rzNFOjO7Pfj/coWZvWBmKV5nkuimMbJ5NEa2jMbH5tH42HwaH7+oTRV0wQ5iDwLnAQOA75jZ\nAG9TRbxa4N+ccwOAUcBNes+abCqw0usQUeQ+4O/OuUJgCHrvvpaZdQFuBYqcc4OAeOAKb1NJNNMY\n2SIaI1tG42PzaHxsBo2PX9WmCjoC3cLWOec2OOeqCeztc5HHmSKac26bc+6T4Of7CPwQ6eJtqshn\nZl2BC4DHvM4SDcwsCxhHsC27c67aObfb21RRIQFINbMEoB2w1eM8Et00RjaTxsjm0/jYPBofW0zj\nYyNtraDrAmxudL8U/eBtMjPrQaC19kJvk0SFe4G7gHqvg0SJnkA58GTwMpzHzCzN61CRzDm3Bfgd\nsAnYBuxxzr3rbSqJchojT4DGyCbT+Ng8Gh+bSePjV7W1gk5ayMzSgVeB25xze73OE8nMbCLgd84t\n9jpLFEkATgH+5JwbBuwHtH7na5hZDoHZk55AZyDNzK72NpVIbNIY2TQaH1tE42MzaXz8qrZW0G0B\nujW63zV4TL6GmSUSGKiec8791es8UWA0MMnMNhK4ZOksM3vW20gRrxQodc41/GX7FQIDmBzb2cBn\nzrly51wN8FfgdI8zSXTTGNkCGiObReNj82l8bD6Nj1/S1gq6j4G+ZtbTzJIILJB8w+NMEc3MjMB1\n2yudc3/wOk80cM79xDnX1TnXg8C/sfedczH9l6Hjcc5tBzabWUHw0DeAEg8jRYNNwCgzaxf8f/oN\ntFBeTozGyGbSGNk8Gh+bT+Nji2h8/JIErwOEknOu1sxuBmYS6HjzhHOu2ONYkW40cA2w3MyWBo/9\nh3PuHQ8zSdt0C/Bc8BfJDcD3Pc4T0ZxzC83sFeATAp32lgDTvU0l0UxjZItojJTWoPGxGTQ+fpU5\n57zOICIiIiIiIi3Q1i65FBERERERiRkq6ERERERERKKUCjoREREREZEopYJOREREREQkSqmgExER\nERERiVIq6ERERERERKKUCjoREREREZEopYJOREREREQkSv1/+hKrzI7OsgYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}